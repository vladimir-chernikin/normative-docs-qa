#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ embeddings –¥–ª—è —É–º–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
"""

import sys
import json
from pathlib import Path
from typing import List, Dict, Any

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ vector-db-test
sys.path.insert(0, str(Path(__file__).parent / "vector-db-test"))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
import numpy as np
import pickle


class SentenceTransformerEmbeddings(Embeddings):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è SentenceTransformer"""

    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        embedding = self.model.encode([text], show_progress_bar=False)[0]
        return embedding.tolist()


class StructuredVectorDB:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""

    def __init__(self, model_name: str = "intfloat/multilingual-e5-small"):
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.embeddings = SentenceTransformerEmbeddings(self.model)

        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ä–∞–∑–º–µ—Ä: {self.model.get_sentence_embedding_dimension()})")

    def create_vector_db(self, chunks: List[Dict[str, Any]], output_path: Path):
        """–°–æ–∑–¥–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ —á–∞–Ω–∫–æ–≤"""

        print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤...")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∞–Ω–∫–∏ –≤ LangChain Document format
        documents = []
        for chunk in chunks:
            doc = Document(
                page_content=chunk['text'],
                metadata=chunk['metadata']
            )
            documents.append(doc)

        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        print(f"üíæ –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        vectorstore = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output_path.mkdir(parents=True, exist_ok=True)

        # FAISS –∏–Ω–¥–µ–∫—Å
        faiss_path = output_path / "index.faiss"
        vectorstore.save_local(str(output_path))

        print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")
        print(f"  - {faiss_path}")
        print(f"  - {output_path / 'index.pkl'}")

        return vectorstore

    def search(self, query: str, vectorstore, top_k: int = 3):
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""

        print(f"üîç –ü–æ–∏—Å–∫: {query}")
        results = vectorstore.similarity_search(query, k=top_k)

        print(f"\n‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:\n")

        for i, result in enumerate(results, 1):
            print(f"–§–†–ê–ì–ú–ï–ù–¢ #{i}")
            print(f"  –î–æ–∫—É–º–µ–Ω—Ç: {result.metadata.get('document', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"  –†–∞–∑–¥–µ–ª: {result.metadata.get('division', '–ù–µ—Ç')}")
            print(f"  –ì–ª–∞–≤–∞: {result.metadata.get('chapter', '–ù–µ—Ç')}")
            print(f"  –°—Ç–∞—Ç—å—è: {result.metadata.get('article', '–ù–µ—Ç')}")
            print(f"  –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤): {result.page_content[:300]}...")
            print()


# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
if __name__ == '__main__':
    # –ü—É—Ç–∏
    chunks_file = Path("/home/olga/normativ_docs/–í–æ–ª–∫–æ–≤/reports/test_chunks_code.json")
    output_dir = Path("/home/olga/normativ_docs/–í–æ–ª–∫–æ–≤/vector-db-test/vectordb/–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π_–∫–æ–¥–µ–∫—Å_–†–§_—á–∞—Å—Ç—å_1_e5")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑: {chunks_file}")
    with open(chunks_file, 'r', encoding='utf-8') as f:
        chunks = json.load(f)

    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    print()

    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    db = StructuredVectorDB()
    vectorstore = db.create_vector_db(chunks, output_dir)

    print()
    print("=" * 80)
    print("–¢–ï–°–¢–û–í–´–ô –ü–û–ò–°–ö")
    print("=" * 80)
    print()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ß–∞—Å—Ç–∏ 2 (–¥–æ–≥–æ–≤–æ—Ä—ã, –∫—É–ø–ª—è-–ø—Ä–æ–¥–∞–∂–∞, –∞—Ä–µ–Ω–¥–∞)
    test_queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–æ–≥–æ–≤–æ—Ä –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏?",
        "–ö–∞–∫–∏–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –µ—Å—Ç—å —É –ø—Ä–æ–¥–∞–≤—Ü–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–æ–≥–æ–≤–æ—Ä –∞—Ä–µ–Ω–¥—ã?",
        "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –µ—Å—Ç—å —É –ø–æ–∫—É–ø–∞—Ç–µ–ª—è?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–æ–≥–æ–≤–æ—Ä –ø–æ—Å—Ç–∞–≤–∫–∏?",
    ]

    for query in test_queries:
        db.search(query, vectorstore, top_k=2)
        print("-" * 80)
        print()
