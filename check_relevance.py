#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
"""

import sys
import json
from pathlib import Path
from typing import List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ vector-db-test
sys.path.insert(0, str(Path(__file__).parent / "vector-db-test"))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings


class SentenceTransformerEmbeddings(Embeddings):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è SentenceTransformer"""

    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, show_progress_bar=False)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        embedding = self.model.encode([text], show_progress_bar=False)[0]
        return embedding.tolist()


def check_relevance():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    vectordb_path = Path("/home/olga/normativ_docs/–í–æ–ª–∫–æ–≤/vector-db-test/vectordb/unified_all_docs_e5")

    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏–∑: {vectordb_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ intfloat/multilingual-e5-small...")
    model = SentenceTransformer("intfloat/multilingual-e5-small")
    embeddings = SentenceTransformerEmbeddings(model)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS
    vectorstore = FAISS.load_local(
        str(vectordb_path),
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    print("‚úì –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏
    test_cases = [
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ?",
            "expected_chapter": "–ì–ª–∞–≤–∞ 4. –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ª–∏—Ü–∞",
            "expected_article": "–°—Ç–∞—Ç—å—è 48. –ü–æ–Ω—è—Ç–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–ª–æ–≥?",
            "expected_chapter": "–ì–ª–∞–≤–∞ 23. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤",
            "expected_article": "–°—Ç–∞—Ç—å—è 334. –ü–æ–Ω—è—Ç–∏–µ –∑–∞–ª–æ–≥–∞"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫ –∏ –∫–∞–∫ –æ–Ω –ø—Ä–µ–¥—ä—è–≤–ª—è–µ—Ç—Å—è?",
            "expected_chapter": "–ì–ª–∞–≤–∞ 12. –ò—Å–∫–æ–≤–∞—è –¥–∞–≤–Ω–æ—Å—Ç—å",
            "expected_article": None  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        },
        {
            "question": "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Å–ø–æ—Å–æ–±—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤?",
            "expected_chapter": "–ì–ª–∞–≤–∞ 23. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤",
            "expected_article": "–°—Ç–∞—Ç—å—è 329. –°–ø–æ—Å–æ–±—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤"
        }
    ]

    print("=" * 80)
    print("–ü–†–û–í–ï–†–ö–ê –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò –ü–û–ò–°–ö–ê")
    print("=" * 80)
    print()

    for i, test_case in enumerate(test_cases, 1):
        question = test_case["question"]
        expected_chapter = test_case["expected_chapter"]
        expected_article = test_case["expected_article"]

        print(f"–¢–ï–°–¢ #{i}")
        print(f"–í–û–ü–†–û–°: {question}")
        print()

        # –ò—â–µ–º —Å k=5 –∏ k=10
        for k in [3, 5, 10]:
            results = vectorstore.similarity_search(question, k=k)

            print(f"üîç –ü–æ–∏—Å–∫ top-{k}:")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–∂–∏–¥–∞–µ–º–∞—è –≥–ª–∞–≤–∞/—Å—Ç–∞—Ç—å—è
            found_chapter = False
            found_article = False

            for j, result in enumerate(results, 1):
                chapter = result.metadata.get('chapter', '')
                article = result.metadata.get('article', '')

                is_expected_chapter = expected_chapter and expected_chapter in chapter
                is_expected_article = expected_article and expected_article == article

                if is_expected_chapter:
                    found_chapter = True
                if is_expected_article:
                    found_article = True

                # –ú–∞—Ä–∫–∏—Ä—É–µ–º –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                marker = ""
                if is_expected_article:
                    marker = " ‚úÖ –û–ñ–ò–î–ê–ï–¢–°–Ø"
                elif is_expected_chapter:
                    marker = " ‚≠ê –û–ñ–ò–î–ê–ï–ú–ê–Ø –ì–õ–ê–í–ê"

                print(f"  {j}. {article}")
                print(f"     {chapter}{marker}")
                print(f"     {result.page_content[:150]}...")
                print()

            # –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
            if expected_article:
                if found_article:
                    print(f"‚úÖ –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–∞—Ç—å—è –ù–ê–ô–î–ï–ù–ê –≤ top-{k}")
                else:
                    print(f"‚ùå –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç–∞—Ç—å—è –ù–ï –ù–ê–ô–î–ï–ù–ê –≤ top-{k}")
            elif expected_chapter:
                if found_chapter:
                    print(f"‚úÖ –û–∂–∏–¥–∞–µ–º–∞—è –≥–ª–∞–≤–∞ –ù–ê–ô–î–ï–ù–ê –≤ top-{k}")
                else:
                    print(f"‚ùå –û–∂–∏–¥–∞–µ–º–∞—è –≥–ª–∞–≤–∞ –ù–ï –ù–ê–ô–î–ï–ù–ê –≤ top-{k}")

            print()

        print("-" * 80)
        print()


if __name__ == '__main__':
    check_relevance()
