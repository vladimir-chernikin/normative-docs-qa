#!/usr/bin/env python3
"""
–£–º–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
"""

import json
import logging
from pathlib import Path
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document

from config import VECTORDB_DIR, EMBEDDING_MODEL

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=False).tolist()

    def embed_query(self, text: str):
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


def load_vector_db():
    db_path = VECTORDB_DIR / "unified_all_docs_e5"
    
    if not db_path.exists():
        logger.error(f"‚ùå –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        return None
    
    logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)
    embeddings = SentenceTransformerEmbeddings(model)
    
    logger.info(f"üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –ë–î –∏–∑ {db_path}")
    vectorstore = FAISS.load_local(str(db_path), embeddings, allow_dangerous_deserialization=True)
    
    return vectorstore


def check_relevance_smart(query, result, expected_keywords):
    """
    –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    
    Returns:
        (is_relevant, confidence, reason)
    """
    metadata = result.metadata
    doc = metadata.get('document', '')
    article = metadata.get('article', '')
    content = result.page_content.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    content_matches = 0
    for keyword in expected_keywords:
        if keyword.lower() in content or keyword.lower() in doc.lower() or keyword.lower() in article.lower():
            content_matches += 1
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    doc_lower = doc.lower()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É
    if any(kw in query.lower() for kw in ['–∂–∫', '–∂–∏–ª–∏—â–Ω—ã–π', '–º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π']):
        expected_doc_type = '–∂–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å'
    elif any(kw in query.lower() for kw in ['–≥–∫', '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π', '—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ']):
        expected_doc_type = '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å'
    elif any(kw in query.lower() for kw in ['–ø—Ä–∞–≤–∏–ª–∞ 354', '–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏', '—Å—á–µ—Ç—á–∏–∫', '–≥–æ—Ä—è—á–∞—è –≤–æ–¥–∞']):
        expected_doc_type = '–ø—Ä–∞–≤–∏–ª ‚Ññ 354'
    elif any(kw in query.lower() for kw in ['–ø—Ä–∞–≤–∏–ª–∞ 491', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞']):
        expected_doc_type = '–ø—Ä–∞–≤–∏–ª ‚Ññ 491'
    else:
        expected_doc_type = None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    doc_match = False
    if expected_doc_type:
        doc_match = expected_doc_type in doc_lower
    
    # –í—ã—á–∏—Å–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    confidence = 0.0
    reasons = []
    
    if content_matches >= len(expected_keywords):
        confidence += 0.5
        reasons.append(f"–í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã ({content_matches}/{len(expected_keywords)})")
    elif content_matches > 0:
        confidence += 0.3 * (content_matches / len(expected_keywords))
        reasons.append(f"–ù–∞–π–¥–µ–Ω–æ {content_matches}/{len(expected_keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
    
    if doc_match:
        confidence += 0.5
        reasons.append(f"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç: {expected_doc_type}")
    elif expected_doc_type and expected_doc_type not in doc_lower:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if '–∂–∏–ª–∏—â–Ω—ã–π' in query.lower() and '–∂–∏–ª–∏—â–Ω—ã–π' in doc_lower:
            confidence += 0.3
            reasons.append("–î–æ–∫—É–º–µ–Ω—Ç –ñ–ö –†–§ (–ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é)")
        elif '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π' in query.lower() and '–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π' in doc_lower:
            confidence += 0.3
            reasons.append("–î–æ–∫—É–º–µ–Ω—Ç –ì–ö –†–§ (–ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é)")
        elif '–ø—Ä–∞–≤–∏–ª' in query.lower() and '‚Ññ 354' in query.lower() and '354' in doc:
            confidence += 0.5
            reasons.append("–î–æ–∫—É–º–µ–Ω—Ç –ü—Ä–∞–≤–∏–ª–∞ 354")
        elif '–ø—Ä–∞–≤–∏–ª' in query.lower() and '‚Ññ 491' in query.lower() and '491' in doc:
            confidence += 0.5
            reasons.append("–î–æ–∫—É–º–µ–Ω—Ç –ü—Ä–∞–≤–∏–ª–∞ 491")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    is_relevant = confidence >= 0.6
    
    return is_relevant, confidence, reasons


def test_relevance_smart(vectorstore):
    """–£–º–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    
    test_cases = [
        {
            "query": "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º?",
            "keywords": ["–º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º", "–∑–¥–∞–Ω–∏–µ", "–∫–≤–∞—Ä—Ç–∏—Ä"],
            "expected_doc": "–ñ–ö –†–§"
        },
        {
            "query": "–ö–∞–∫–∏–µ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç?",
            "keywords": ["–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏", "–≤–æ–¥–∞", "–≥–∞–∑", "—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∞—è", "–æ—Ç–æ–ø–ª–µ–Ω–∏–µ"],
            "expected_doc": "–ñ–ö –†–§"
        },
        {
            "query": "–ü—Ä–∞–≤–∏–ª–∞ —Å–Ω—è—Ç–∏—è –ø–æ–∫–∞–∑–∞–Ω–∏–π —Å—á–µ—Ç—á–∏–∫–æ–≤",
            "keywords": ["—Å—á–µ—Ç—á–∏–∫", "–ø–æ–∫–∞–∑–∞–Ω–∏", "–ø—Ä–∏–±–æ—Ä —É—á–µ—Ç"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354"
        },
        {
            "query": "–ö—Ç–æ –ø–ª–∞—Ç–∏—Ç –∑–∞ –æ–±—â–µ–¥–æ–º–æ–≤—ã–µ –Ω—É–∂–¥—ã?",
            "keywords": ["–æ–±—â–µ–¥–æ–º–æ–≤—ã–µ –Ω—É–∂–¥—ã", "–ø–ª–∞—Ç–∞", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354"
        },
        {
            "query": "–ö–∞–∫ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤?",
            "keywords": ["–æ–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫", "–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ"],
            "expected_doc": "–ñ–ö –†–§"
        },
        {
            "query": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–∞–ø–∏—Ç–∞–ª—å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç?",
            "keywords": ["–∫–∞–ø–∏—Ç–∞–ª—å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç", "–æ–±—â–µ–µ –∏–º—É—â–µ—Å—Ç–≤–æ"],
            "expected_doc": "–ñ–ö –†–§"
        },
        {
            "query": "–ü—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ø–æ–º–µ—â–µ–Ω–∏—è",
            "keywords": ["—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫", "–ø—Ä–∞–≤–∞", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç", "–ø–æ–º–µ—â–µ–Ω–∏–µ"],
            "expected_doc": "–ñ–ö –†–§ –∏–ª–∏ –ì–ö –†–§"
        },
        {
            "query": "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–ª–∞—Ç–∞ –∑–∞ –æ—Ç–æ–ø–ª–µ–Ω–∏–µ?",
            "keywords": ["–æ—Ç–æ–ø–ª", "—Ä–∞—Å—á–µ—Ç", "–ø–ª–∞—Ç–∞", "–∫–æ–º–º—É–Ω–∞–ª—å–Ω–∞—è —É—Å–ª—É–≥–∞"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354"
        },
        {
            "query": "–ß—Ç–æ –≤—Ö–æ–¥–∏—Ç –≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞?",
            "keywords": ["–æ–±—â–µ–µ –∏–º—É—â–µ—Å—Ç–≤–æ", "—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ", "–º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 491"
        },
        {
            "query": "–ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä –ø–ª–∞—Ç—ã –∑–∞ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏?",
            "keywords": ["—Ä–∞–∑–º–µ—Ä –ø–ª–∞—Ç—ã", "–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏", "–∏–∑–º–µ–Ω–µ–Ω"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354 –∏–ª–∏ –ñ–ö –†–§"
        },
        {
            "query": "–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç –≥–æ—Ä—è—á–µ–π –≤–æ–¥—ã?",
            "keywords": ["–≥–æ—Ä—è—á–∞—è –≤–æ–¥–∞", "–∫–æ–º–º—É–Ω–∞–ª—å–Ω–∞—è —É—Å–ª—É–≥–∞", "–Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354"
        },
        {
            "query": "–ö—Ç–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–º –¥–æ–º–æ–º?",
            "keywords": ["—É–ø—Ä–∞–≤–ª–µ–Ω", "–º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º", "—É–ø—Ä–∞–≤–ª—è—é—â–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü"],
            "expected_doc": "–ñ–ö –†–§"
        },
        {
            "query": "–ü–æ—Ä—è–¥–æ–∫ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –ø–µ–Ω–∏ –∑–∞ –ø—Ä–æ—Å—Ä–æ—á–∫—É –ø–ª–∞—Ç–µ–∂–∞",
            "keywords": ["–ø–µ–Ω", "–ø—Ä–æ—Å—Ä–æ—á–∫", "–ø–ª–∞—Ç–µ–∂", "–Ω–µ—É—Å—Ç–æ–π–∫–∞"],
            "expected_doc": "–ì–ö –†–§ –∏–ª–∏ –ñ–ö –†–§"
        },
        {
            "query": "–ö–∞–∫ –æ—Å–ø–æ—Ä–∏—Ç—å –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –ø–æ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–º —É—Å–ª—É–≥–∞–º?",
            "keywords": ["–æ—Å–ø–æ—Ä–∏—Ç—å", "–Ω–∞—á–∏—Å–ª–µ–Ω", "–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏", "–ø—Ä–µ—Ç–µ–Ω–∑"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354"
        },
        {
            "query": "–ß—Ç–æ —Ç–∞–∫–æ–µ –ª–∏—Ü–µ–≤–æ–π —Å—á–µ—Ç?",
            "keywords": ["–ª–∏—Ü–µ–≤–æ–π —Å—á–µ—Ç", "–ø–ª–∞—Ç–µ–∂", "—Å—á–µ—Ç"],
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 354 –∏–ª–∏ –ñ–ö –†–§"
        },
    ]
    
    results_summary = []
    
    for i, test_case in enumerate(test_cases, 1):
        query = test_case["query"]
        keywords = test_case["keywords"]
        expected_doc = test_case["expected_doc"]
        
        logger.info("\n" + "=" * 100)
        logger.info(f"–¢–ï–°–¢ #{i}: {query}")
        logger.info(f"–û–∂–∏–¥–∞–µ—Ç—Å—è: {expected_doc}")
        logger.info(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}")
        logger.info("=" * 100)
        
        results = vectorstore.similarity_search(query, k=3)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for j, result in enumerate(results, 1):
            metadata = result.metadata
            doc = metadata.get('document', 'Unknown')
            article = metadata.get('article', '')
            logger.info(f"\n{j}. üìÑ {doc}")
            logger.info(f"   üìú {article}")
            logger.info(f"   üìñ {result.page_content[:120]}...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        top_result = results[0]
        is_relevant, confidence, reasons = check_relevance_smart(query, top_result, keywords)
        
        if is_relevant:
            relevance_mark = "‚úÖ –†–ï–õ–ï–í–ê–ù–¢–ù–û"
        elif confidence >= 0.4:
            relevance_mark = "‚ö†Ô∏è –ß–ê–°–¢–ò–ß–ù–û –†–ï–õ–ï–í–ê–ù–¢–ù–û"
        else:
            relevance_mark = "‚ùå –ù–ï –†–ï–õ–ï–í–ê–ù–¢–ù–û"
        
        logger.info(f"\n{relevance_mark} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
        logger.info(f"–ü—Ä–∏—á–∏–Ω—ã: {', '.join(reasons)}")
        logger.info("-" * 100)
        
        results_summary.append({
            'test_num': i,
            'query': query,
            'expected_doc': expected_doc,
            'actual_doc': f"{results[0].metadata.get('document', '')} {results[0].metadata.get('article', '')}",
            'is_relevant': is_relevant,
            'confidence': round(confidence, 2),
            'reasons': reasons,
            'relevance_mark': relevance_mark
        })
    
    return results_summary


def main():
    logger.info("=" * 100)
    logger.info("–£–ú–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò")
    logger.info("=" * 100)
    
    vectorstore = load_vector_db()
    if not vectorstore:
        return
    
    results_summary = test_relevance_smart(vectorstore)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_path = Path(__file__).parent / "reports" / "relevance_test_smart.json"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total = len(results_summary)
    relevant = sum(1 for r in results_summary if r['is_relevant'])
    partial = sum(1 for r in results_summary if not r['is_relevant'] and r['confidence'] >= 0.4)
    not_relevant = total - relevant - partial
    
    avg_confidence = sum(r['confidence'] for r in results_summary) / total
    
    logger.info("\n" + "=" * 100)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info("=" * 100)
    logger.info(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
    logger.info(f"‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ: {relevant} ({relevant/total*100:.1f}%)")
    logger.info(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ: {partial} ({partial/total*100:.1f}%)")
    logger.info(f"‚ùå –ù–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ: {not_relevant} ({not_relevant/total*100:.1f}%)")
    logger.info(f"\nüìä –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2f}")
    logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    logger.info("=" * 100)


if __name__ == '__main__':
    main()
