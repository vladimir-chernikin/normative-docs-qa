#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ embedding scores –¥–ª—è –ø–æ–∏—Å–∫–∞
"""

import sys
import numpy as np
from pathlib import Path
from typing import List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ vector-db-test
sys.path.insert(0, str(Path(__file__).parent / "vector-db-test"))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings


class SentenceTransformerEmbeddings(Embeddings):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è SentenceTransformer"""

    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        embeddings = self.model.encode(texts, show_progress_bar=False)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        embedding = self.model.encode([text], show_progress_bar=False)[0]
        return embedding.tolist()


def check_scores():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç embedding scores"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    vectordb_path = Path("/home/olga/normativ_docs/–í–æ–ª–∫–æ–≤/vector-db-test/vectordb/unified_all_docs_e5")

    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏–∑: {vectordb_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ intfloat/multilingual-e5-small...")
    model = SentenceTransformer("intfloat/multilingual-e5-small")
    embeddings = SentenceTransformerEmbeddings(model)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS
    vectorstore = FAISS.load_local(
        str(vectordb_path),
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    print("‚úì –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å "–ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–ª–æ–≥?"
    question = "–ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–ª–æ–≥?"
    print(f"–í–û–ü–†–û–°: {question}")
    print()

    # –î–µ–ª–∞–µ–º –ø–æ–∏—Å–∫ —Å scores
    results_with_scores = vectorstore.similarity_search_with_score(question, k=15)

    print("–¢–û–ü-15 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –° SCORES:")
    print("=" * 80)

    for i, (doc, score) in enumerate(results_with_scores, 1):
        article = doc.metadata.get('article', '–ù–µ—Ç')
        chapter = doc.metadata.get('chapter', '–ù–µ—Ç')

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º score –∏–∑ –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ
        # FAISS –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç L2 –¥–∏—Å—Ç–∞–Ω—Ü–∏—é, —á–µ–º –º–µ–Ω—å—à–µ - —Ç–µ–º –ª—É—á—à–µ
        # –î–ª—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 1 / (1 + distance)
        similarity = 1 / (1 + score)

        marker = ""
        if '–°—Ç–∞—Ç—å—è 334' in article:
            marker = " ‚úÖ –ò–°–ö–û–ú–ê–Ø –°–¢–ê–¢–¨–Ø!"
        elif '–ì–ª–∞–≤–∞ 23' in chapter:
            marker = " ‚≠ê –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –≥–ª–∞–≤–∞"

        print(f"{i}. {article}")
        print(f"   {chapter}")
        print(f"   Score (L2 distance): {score:.4f}")
        print(f"   Similarity: {similarity:.4f}{marker}")
        print(f"   –¢–µ–∫—Å—Ç: {doc.page_content[:100]}...")
        print()


if __name__ == '__main__':
    check_scores()
