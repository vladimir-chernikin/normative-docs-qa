#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
"""

from pathlib import Path
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from typing import List


class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=False).tolist()

    def embed_query(self, text: str):
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


def test_unified_db():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –µ–¥–∏–Ω—É—é –ë–î"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ë–î
    db_path = Path("vector-db-test/vectordb/unified_all_docs_e5")
    
    print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    model = SentenceTransformer("intfloat/multilingual-e5-small")
    embeddings = SentenceTransformerEmbeddings(model)
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ë–î –∏–∑: {db_path}")
    vectorstore = FAISS.load_local(str(db_path), embeddings=embeddings, allow_dangerous_deserialization=True)
    print("‚úì –ë–î –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")

    # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞–∑–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    test_questions = [
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ?",
            "expected_doc": "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å",
            "expected_article": "–°—Ç–∞—Ç—å—è 48"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–ª–æ–≥?",
            "expected_doc": "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å",
            "expected_article": "–°—Ç–∞—Ç—å—è 334"
        },
        {
            "question": "–ö–∞–∫–∏–µ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è?",
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ ‚Ññ 354",
            "expected_section": "–•–æ–ª–æ–¥–Ω–æ–µ –≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ"
        },
        {
            "question": "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–ª–∞—Ç–∞ –∑–∞ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏?",
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ ‚Ññ 354"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º?",
            "expected_doc": "–ñ–∏–ª–∏—â—ã–π –∫–æ–¥–µ–∫—Å"
        },
        {
            "question": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –µ—Å—Ç—å —É —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ø–æ–º–µ—â–µ–Ω–∏—è?",
            "expected_doc": "–ñ–∏–ª–∏—â—ã–π –∫–æ–¥–µ–∫—Å"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—â–µ–µ –∏–º—É—â–µ—Å—Ç–≤–æ –≤ –ú–ö–î?",
            "expected_doc": "–ü—Ä–∞–≤–∏–ª–∞ 491"
        },
        {
            "question": "–ö–∞–∫ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤?",
            "expected_doc": "–ü—Ä–∏–∫–∞–∑ 44"
        },
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —ç–Ω–µ—Ä–≥–æ—Å–±–µ—Ä–µ–∂–µ–Ω–∏–µ?",
            "expected_doc": "261-–§–ó"
        },
        {
            "question": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ?",
            "expected_doc": "152-–§–ó"
        }
    ]

    print("=" * 80)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ï–î–ò–ù–û–ô –ë–î")
    print("=" * 80)
    print(f"–í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(test_questions)}\n")

    for i, test in enumerate(test_questions, 1):
        print(f"\n[–¢–ï–°–¢ {i}/{len(test_questions)}]")
        print(f"–í–û–ü–†–û–°: {test['question']}")
        
        if 'expected_doc' in test:
            print(f"–û–ñ–ò–î–ê–ï–¢–°–Ø: {test.get('expected_doc', '–õ—é–±–æ–π')}")
        
        # –ü–æ–∏—Å–∫
        results = vectorstore.similarity_search_with_score(test['question'], k=3)
        
        print(f"\n–†–ï–ó–£–õ–¨–¢–ê–¢–´ (top-3):")
        print("-" * 80)

        for j, (doc, score) in enumerate(results, 1):
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º score –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = 1 / (1 + score)
            
            doc_name = doc.metadata.get('document', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            article = doc.metadata.get('article', '')
            chapter = doc.metadata.get('chapter', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è–º
            expected = test.get('expected_doc', '')
            match = "‚úÖ" if expected and expected.lower() in doc_name.lower() else ""
            
            print(f"{j}. [{doc_name}] {match}")
            if article:
                print(f"   {article}")
            elif chapter:
                print(f"   {chapter}")
            print(f"   –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.4f}")
            print(f"   –¢–µ–∫—Å—Ç: {doc.page_content[:120]}...")
            print()

        print("-" * 80)

    print("\n‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")


if __name__ == '__main__':
    test_unified_db()
