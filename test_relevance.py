#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
"""

import json
import logging
from pathlib import Path
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document

from config import VECTORDB_DIR, EMBEDDING_MODEL

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=False).tolist()

    def embed_query(self, text: str):
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


def load_vector_db():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
    db_path = VECTORDB_DIR / "unified_all_docs_e5"
    
    if not db_path.exists():
        logger.error(f"‚ùå –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_path}")
        return None
    
    logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)
    embeddings = SentenceTransformerEmbeddings(model)
    
    logger.info(f"üíæ –ó–∞–≥—Ä—É–∑–∫–∞ –ë–î –∏–∑ {db_path}")
    vectorstore = FAISS.load_local(str(db_path), embeddings, allow_dangerous_deserialization=True)
    
    return vectorstore


def format_result(result: Document, rank: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
    metadata = result.metadata
    doc = metadata.get('document', 'Unknown')
    article = metadata.get('article', '')
    level = metadata.get('level', 'N/A')
    
    return f"""
    {rank}. üìÑ {doc}
       üìú {article}
       üìä Level: {level}
       üìñ {result.page_content[:150]}...
    """


def test_relevance(vectorstore):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å"""
    
    test_queries = [
        ("–ß—Ç–æ —Ç–∞–∫–æ–µ –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è 15"),
        ("–ö–∞–∫–∏–µ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è 154"),
        ("–ü—Ä–∞–≤–∏–ª–∞ —Å–Ω—è—Ç–∏—è –ø–æ–∫–∞–∑–∞–Ω–∏–π —Å—á–µ—Ç—á–∏–∫–æ–≤", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
        ("–ö—Ç–æ –ø–ª–∞—Ç–∏—Ç –∑–∞ –æ–±—â–µ–¥–æ–º–æ–≤—ã–µ –Ω—É–∂–¥—ã?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è 154"),
        ("–ö–∞–∫ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±—â–µ–µ —Å–æ–±—Ä–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è 44-48"),
        ("–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–∞–ø–∏—Ç–∞–ª—å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è..."),
        ("–ü—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ø–æ–º–µ—â–µ–Ω–∏—è", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è..."),
        ("–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–ª–∞—Ç–∞ –∑–∞ –æ—Ç–æ–ø–ª–µ–Ω–∏–µ?", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
        ("–ß—Ç–æ –≤—Ö–æ–¥–∏—Ç –≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞?", "–ü—Ä–∞–≤–∏–ª–∞ 491"),
        ("–ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä –ø–ª–∞—Ç—ã –∑–∞ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏?", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
        ("–ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –Ω–µ—Ç –≥–æ—Ä—è—á–µ–π –≤–æ–¥—ã?", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
        ("–ö—Ç–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–º –¥–æ–º–æ–º?", "–ñ–ö –†–§ –°—Ç–∞—Ç—å—è 161"),
        ("–ü–æ—Ä—è–¥–æ–∫ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –ø–µ–Ω–∏ –∑–∞ –ø—Ä–æ—Å—Ä–æ—á–∫—É –ø–ª–∞—Ç–µ–∂–∞", "–ì–ö –†–§ –°—Ç–∞—Ç—å—è 330"),
        ("–ö–∞–∫ –æ—Å–ø–æ—Ä–∏—Ç—å –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –ø–æ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–º —É—Å–ª—É–≥–∞–º?", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
        ("–ß—Ç–æ —Ç–∞–∫–æ–µ –ª–∏—Ü–µ–≤–æ–π —Å—á–µ—Ç?", "–ü—Ä–∞–≤–∏–ª–∞ 354"),
    ]
    
    results_summary = []
    
    for i, (query, expected_doc) in enumerate(test_queries, 1):
        logger.info("\n" + "=" * 100)
        logger.info(f"–¢–ï–°–¢ #{i}: {query}")
        logger.info(f"–û–∂–∏–¥–∞–µ—Ç—Å—è: {expected_doc}")
        logger.info("=" * 100)
        
        results = vectorstore.similarity_search(query, k=3)
        
        for j, result in enumerate(results, 1):
            logger.info(format_result(result, j))
        
        top_doc = results[0].metadata.get('document', '')
        top_article = results[0].metadata.get('article', '')
        
        relevance = "‚úÖ –†–ï–õ–ï–í–ê–ù–¢–ù–û" if expected_doc.split()[0] in top_doc else "‚ö†Ô∏è –ü–†–û–í–ï–†–ò–¢–¨"
        
        logger.info(f"\n{relevance}")
        logger.info("-" * 100)
        
        results_summary.append({
            'query': query,
            'expected': expected_doc,
            'actual': f"{top_doc} {top_article}",
            'relevance': relevance
        })
    
    return results_summary


def main():
    logger.info("=" * 100)
    logger.info("–î–ï–¢–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–õ–ï–í–ê–ù–¢–ù–û–°–¢–ò")
    logger.info("=" * 100)
    
    vectorstore = load_vector_db()
    if not vectorstore:
        return
    
    results_summary = test_relevance(vectorstore)
    
    report_path = Path(__file__).parent / "reports" / "relevance_test_detailed.json"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, ensure_ascii=False, indent=2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    relevant = sum(1 for r in results_summary if "‚úÖ" in r['relevance'])
    total = len(results_summary)
    
    logger.info("\n" + "=" * 100)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info("=" * 100)
    logger.info(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total}")
    logger.info(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ: {relevant}")
    logger.info(f"–¢–æ—á–Ω–æ—Å—Ç—å: {relevant/total*100:.1f}%")
    logger.info(f"\n‚úÖ –û—Ç—á–µ—Ç: {report_path}")
    logger.info("=" * 100)


if __name__ == '__main__':
    main()
