#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –¥–ª—è –≤—Å–µ—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import json
import logging
from pathlib import Path
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from typing import List

from config import FULLDOCS_DIR, VECTORDB_DIR, EMBEDDING_MODEL
from universal_chunker import UniversalDocumentChunker

logger = logging.getLogger(__name__)


class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=True).tolist()

    def embed_query(self, text: str):
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


def create_unified_db():
    """–°–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—É—é –ë–î –∏–∑ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ DOCX —Ñ–∞–π–ª—ã
    docx_files = sorted(FULLDOCS_DIR.glob("*.docx"))
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docx_files)}")

    all_chunks = []

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    for i, docx_file in enumerate(docx_files, 1):
        print(f"\n[{i}/{len(docx_files)}] {docx_file.name}")

        structure_file = docx_file.with_name(docx_file.stem + '_structure.txt')

        if not structure_file.exists():
            print(f"   ‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        try:
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
            chunker = UniversalDocumentChunker(docx_file, structure_file)
            chunks = chunker.extract_chunks()
            
            print(f"   ‚úì {len(chunks)} —á–∞–Ω–∫–æ–≤")
            all_chunks.extend(chunks)

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            continue

    print(f"\nüìä –í–°–ï–ì–û –ß–ê–ù–ö–û–í: {len(all_chunks)}")

    # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è LangChain
    documents = []
    for chunk in all_chunks:
        doc = Document(
            page_content=chunk['text'],
            metadata=chunk['metadata']
        )
        documents.append(doc)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)
    embeddings = SentenceTransformerEmbeddings(model)
    print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
    print(f"üíæ –°–æ–∑–¥–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–π –ë–î –∏–∑ {len(documents)} —á–∞–Ω–∫–æ–≤...")
    vectorstore = FAISS.from_documents(documents, embedding=embeddings)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    output_dir = VECTORDB_DIR / "unified_all_docs_e5"
    output_dir.mkdir(parents=True, exist_ok=True)
    vectorstore.save_local(str(output_dir))

    print(f"‚úì –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_dir}")

    # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    print("\n" + "=" * 80)
    print("–¢–ï–°–¢–û–í–´–ô –ü–û–ò–°–ö")
    print("=" * 80)

    test_queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ?",
        "–ö–∞–∫–∏–µ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª—É–≥–∏?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º?",
    ]

    for query in test_queries:
        print(f"\nüîç {query}")
        results = vectorstore.similarity_search(query, k=2)
        
        for i, r in enumerate(results, 1):
            doc = r.metadata.get('document', 'Unknown')
            art = r.metadata.get('article', '')
            print(f"   {i}. [{doc}] {art}")
            print(f"      {r.page_content[:100]}...")

    print("\n‚úÖ –ì–û–¢–û–í–û!")


if __name__ == '__main__':
    create_unified_db()
