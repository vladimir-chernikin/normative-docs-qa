# ИНСТРУКЦИЯ: Работа с нормативными документами

**Дата:** 2025-01-07
**Директория:** `/home/olga/normativ_docs/Волков/`

---

## Скрипты для работы с документами

1. **universal_chunker.py** - разбивает документ на чанки (сохраняет в JSON)
2. **check_chunk_quality.py** - проверка качества чанков (опционально)
3. **document_processor.py** - создает векторную базу (разбивает + embeddings + FAISS)

---

## Шаг 1: Подготовка документа

**Документы лежат в директории:** `fulldocx/`

Проверить список документов:
```bash
cd /home/olga/normativ_docs/Волков
ls -lh fulldocx/*.docx
```

**Примеры документов:**
- Гражданский кодекс Российской Федерации (часть первая).docx
- Жилищный кодекс Российской Федерации.docx
- ПП РФ от 06.05.2011 № 354 Правила предоставления коммунальных услуг (Правила № 354).docx

---

## Шаг 2: Тест разбиения на чанки

**Скрипт:** `universal_chunker.py`

**Для чего:** Разбивает документ на чанки и сохраняет в JSON для проверки качества

**Как запускается:**
```bash
python3 universal_chunker.py "Название документа.docx"
```

**Что делает:**
- Читает DOCX файл и *_structure.txt
- Определяет тип документа (CODE, GOVERNMENT_DECREE, MINISTRY_ORDER, LETTER)
- Разбивает по структуре (статьи, пункты, приложения)
- Ограничивает размер чанков (MAX 1000 символов для постановлений)
- Рекурсивно разбивает огромные чанки по предложениям
- Сохраняет чанки в `reports/test_chunks_<тип>.json`
- Показывает в консоли первые 3 чанка с метаданными

**Зачем нужен:** Проверить как разбивается документ ПЕРЕД созданием векторной базы

---

## Шаг 3: Проверка качества чанков (опционально)

**Скрипт:** `check_chunk_quality.py`

**Для чего:** Показывает статистику по чанкам - размер, распределение, проблемы

**Как запускается:**
```bash
python check_chunk_quality.py <chunks.json> [имя_документа]
```

**Примеры:**
```bash
python check_chunk_quality.py reports/test_chunks_government_decree.json "ПП РФ № 354"
```

**Что показывает:**
- Всего чанков
- Минимальный/максимальный/средний/медианный размер
- Распределение по категориям
- Обнаруженные проблемы
- Рекомендации

**Статусы качества:**
- ✅ ОТЛИЧНО - нет проблем
- ✓ ХОРОШО - минимальные проблемы (< 5%)
- ⚠ ДОСТАТОЧНО - есть проблемы (< 15%)
- ❌ ПЛОХО - много проблем (> 15%)

**Зачем нужен:** Убедиться что чанки хорошего размера ПЕРЕД созданием векторной базы

---

## Почему важен размер чанков?

**Слишком маленькие (< 50 символов):**
- Мало контекста, непонятно о чем речь
- Плохой поиск, неточная релевантность

**Слишком большие (> 2000 символов):**
- Трудно найти конкретное
- LLM не обработает (ограничение токены)
- Поиск приводит к общим фразам

**Оптимальные (200-1000 символов):**
- Достаточно контекста для понимания
- Точность поиска высокая
- LLM легко обработает

**Хороший результат:**
- Средний размер: 300-800 символов
- > 70% чанков в диапазоне 200-1000
- Нет слишком мелких (< 50) и крупных (> 2000)

---

## Шаг 4: Создание векторной базы

**Скрипт:** `document_processor.py`

**Для чего:** Создает финальную векторную базу для поиска по одному документу

**Как запускается:**
```bash
# Интерактивно - выбираете из списка
python3 document_processor.py

# Или указываете документ напрямую
python3 document_processor.py "Название документа.docx"
```

**Что делает:**
- Разбивает на чанки (использует UniversalDocumentChunker)
- Создает embeddings (модель `intfloat/multilingual-e5-base`)
- Проверяет качество чанков
- Сохраняет FAISS индекс в `vector-db-test/vectordb/Имя_Документа/`
- Делает тестовый поиск

**Результат:**
- Отдельная векторная база для одного документа
- Готова к использованию для поиска

**Зачем нужен:** Создать рабочую векторную базу для поиска по документу

---
