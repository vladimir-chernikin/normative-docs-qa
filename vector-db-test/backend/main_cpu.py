#!/usr/bin/env python3
"""
Backend API —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import logging
import httpx
import asyncio
import json
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv(project_root / ".env")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
VECTORDB_DIR = project_root / "vectordb"
EMBEDDING_MODEL = "intfloat/multilingual-e5-base"

# YandexGPT –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
YANDEX_API_KEY = os.getenv("YANDEX_API_KEY", "")
YANDEX_FOLDER_ID = os.getenv("YANDEX_FOLDER_ID", "")


async def reformulate_query(original_query: str) -> str:
    """
    –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Ç–µ—Ä–º–∏–Ω—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ñ–ö–•
    –∏—Å–ø–æ–ª—å–∑—É—è YandexGPT –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞.
    """
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        # –ï—Å–ª–∏ –Ω–µ—Ç API –∫–ª—é—á–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        logger.info("‚ö†Ô∏è –ù–µ—Ç Yandex API –∫–ª—é—á–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
        return original_query

    prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –ñ–ö–• –†–§.
–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏–∑ –ñ–ö –†–§, –ü—Ä–∞–≤–∏–ª 354, –ü—Ä–∞–≤–∏–ª 491
2. –í–°–ï–ì–î–ê –ø—Ä–∏–≤–æ–¥–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º —Ç–µ—Ä–º–∏–Ω–∞–º:
   - "—Å–º–µ–Ω–∏—Ç—å –£–ö" ‚Üí "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–º –¥–æ–º–æ–º"
   - "–∫–∞–∫ —Å–º–µ–Ω–∏—Ç—å —É–ø—Ä–∞–≤–ª—è—é—â—É—é –∫–æ–º–ø–∞–Ω–∏—é" ‚Üí "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"
   - "–∫—Ç–æ –ø–ª–∞—Ç–∏—Ç" ‚Üí "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å –ø–æ –≤–Ω–µ—Å–µ–Ω–∏—é –ø–ª–∞—Ç—ã, —Ä–∞–∑–º–µ—Ä –ø–ª–∞—Ç—ã"
   - "–Ω–∞—á–∏—Å–ª–µ–Ω–∏—è" ‚Üí "—Ä–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–ª–∞—Ç—ã"
   - "–ø–ª–∞—Ç–∏—Ç—å –∑–∞ —Å–≤–µ—Ç" ‚Üí "–∫–æ–º–º—É–Ω–∞–ª—å–Ω–∞—è —É—Å–ª—É–≥–∞ –ø–æ —ç–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏—é"
   - "–º–∫–¥" ‚Üí "–º–Ω–æ–≥–æ–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π –¥–æ–º"
   - "–±–∞—Ç–∞—Ä–µ–∏" ‚Üí "–æ—Ç–æ–ø–ª–µ–Ω–∏–µ"
   - "–ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–±—Ä–∞–Ω–∏—è" ‚Üí "–ø—Ä–æ—Ç–æ–∫–æ–ª –æ–±—â–µ–≥–æ —Å–æ–±—Ä–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤"
   - "–∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è" ‚Üí "–ø–æ—Ä—è–¥–æ–∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è"
   - "—á—Ç–æ —Ç–∞–∫–æ–µ" ‚Üí "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø–æ–Ω—è—Ç–∏–µ"
   - "—á—Ç–æ –≤—Ö–æ–¥–∏—Ç" ‚Üí "—Å–æ—Å—Ç–∞–≤, –ø–µ—Ä–µ—á–µ–Ω—å, –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è"
3. –°–æ—Ö—Ä–∞–Ω—è–π —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–∞
4. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ —Å—Ç–∞—Ç–µ–π –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∞–∫—Ç–æ–≤
5. –î–æ–±–∞–≤–ª—è–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞:
   - "–æ–±—â–µ–µ –∏–º—É—â–µ—Å—Ç–≤–æ" ‚Üí –¥–æ–±–∞–≤–ª—è–π "–ø–æ–º–µ—â–µ–Ω–∏—è –æ–±—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –Ω–µ—Å—É—â–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
   - "–ø—Ä–∞–≤–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞" ‚Üí –¥–æ–±–∞–≤–ª—è–π "–ø—Ä–∞–≤–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏, –≤–ª–∞–¥–µ–Ω–∏–µ, –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ, —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ"
   - "–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏" ‚Üí –¥–æ–±–∞–≤–ª—è–π "—Ä–∞–∑–º–µ—Ä –ø–ª–∞—Ç—ã, —Ç–∞—Ä–∏—Ñ—ã, –Ω–æ—Ä–º–∞—Ç–∏–≤—ã, —Ä–∞—Å—á–µ—Ç"
   - "–≤—Ö–æ–¥–∏—Ç –≤ —Å–æ—Å—Ç–∞–≤" ‚Üí –¥–æ–±–∞–≤–ª—è–π "—Å–æ—Å—Ç–∞–≤, –ø–µ—Ä–µ—á–µ–Ω—å, –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è"
6. –û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {original_query}

–£–õ–£–ß–®–ï–ù–ù–´–ô –í–û–ü–†–û–°:"""

    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    headers = {
        "Authorization": f"Api-Key {YANDEX_API_KEY}",
        "Content-Type": "application/json"
    }

    body = {
        "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite",
        "completionOptions": {
            "stream": False,
            "temperature": 0.3,
            "maxTokens": 100
        },
        "messages": [
            {
                "role": "user",
                "text": prompt
            }
        ]
    }

    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()

            # YandexGPT –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç result.alternatives[0].message.text
            reformulated = result["result"]["alternatives"][0]["message"]["text"].strip()

            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –∏ –ª–∏—à–Ω–µ–µ
            reformulated = reformulated.strip('"').strip("'").strip()

            logger.info(f"üîÑ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: '{original_query}' ‚Üí '{reformulated}'")
            return reformulated

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        return original_query


async def rerank_results_with_llm(query: str, results_with_scores: list) -> list:
    """
    –†–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ YandexGPT –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª—É—á—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞.

    Args:
        query: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        results_with_scores: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (doc, score)

    Returns:
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (doc, score) –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    """
    if not results_with_scores:
        return results_with_scores

    # –ï—Å–ª–∏ –≤—Å–µ–≥–æ 1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –Ω–µ—á–µ–≥–æ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å
    if len(results_with_scores) == 1:
        return results_with_scores

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    results_text = ""
    for i, (doc, score) in enumerate(results_with_scores, 1):
        similarity = 1 / (1 + score)
        doc_name = doc.metadata.get('document', 'Unknown')
        article = doc.metadata.get('article', '')
        content_preview = doc.page_content[:300].replace('\n', ' ')

        results_text += f"\n{i}. –î–æ–∫—É–º–µ–Ω—Ç: {doc_name}\n"
        results_text += f"   –°—Ç–∞—Ç—å—è: {article}\n"
        results_text += f"   –°—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2%}\n"
        results_text += f"   –¢–µ–∫—Å—Ç: {content_preview}...\n"

    prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –†–§.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π, –∫–∞–∫–æ–π –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –õ–£–ß–®–ï –í–°–ï–ì–û –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{query}

–ù–ê–ô–î–ï–ù–ù–´–ï –§–†–ê–ì–ú–ï–ù–¢–´:
{results_text}

–ö–†–ò–¢–ï–†–ò–ò –û–¶–ï–ù–ö–ò:
1. –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø–µ—Ä–µ—á–µ–Ω—å, –ø–æ—Ä—è–¥–æ–∫)
2. –ü–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞ (—á–µ–º –ø–æ–ª–Ω–µ–µ, —Ç–µ–º –ª—É—á—à–µ)
3. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (–Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å)
4. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (—Å—Ç–∞—Ç—å—è –∑–∞–∫–æ–Ω–∞ –ª—É—á—à–µ —á–µ–º –ø–∏—Å—å–º–æ)

–£–ö–ê–ñ–ò –ù–û–ú–ï–† –õ–£–ß–®–ï–ì–û –§–†–ê–ì–ú–ï–ù–¢–ê (—Ü–∏—Ñ—Ä–æ–π –æ—Ç 1 –¥–æ {len(results_with_scores)}):
"""

    try:
        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Authorization": f"Api-Key {YANDEX_API_KEY}",
            "Content-Type": "application/json"
        }

        body = {
            "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite",
            "completionOptions": {
                "stream": False,
                "temperature": 0.3,
                "maxTokens": 50
            },
            "messages": [
                {
                    "role": "user",
                    "text": prompt
                }
            ]
        }

        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()

            llm_answer = result["result"]["alternatives"][0]["message"]["text"].strip()

            logger.info(f"üìù –û—Ç–≤–µ—Ç YandexGPT: {llm_answer}")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
            import re
            match = re.search(r'\b([1-9]|1[0-9]|20)\b', llm_answer)

            if match:
                best_idx = int(match.group(1)) - 1  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ 0-based index
                best_idx = max(0, min(best_idx, len(results_with_scores) - 1))

                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ —Å –≤—ã—Å–æ–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π
                best_doc, best_score = results_with_scores.pop(best_idx)
                # –î–∞–µ–º –≤—ã—Å–æ–∫—É—é –æ—Ü–µ–Ω–∫—É 0.99 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É LLM
                results_with_scores.insert(0, (best_doc, 0.99))

                logger.info(f"üéØ LLM —Ä–µ—Ä–∞–Ω–∂–∏–Ω–≥: –≤—ã–±—Ä–∞–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç #{best_idx + 1} –∫–∞–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π (–æ—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {best_score:.3f} -> 0.99)")
            else:
                logger.warning(f"‚ö†Ô∏è LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä: '{llm_answer}'")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM —Ä–µ—Ä–∞–Ω–∂–∏–Ω–≥–∞: {e}")

    return results_with_scores


class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=False).tolist()

    def embed_query(self, text: str):
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


class QueryRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5


class SearchResult(BaseModel):
    document: str
    article: str
    content: str
    metadata: Dict[str, Any]
    similarity: float  # –î–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–ª–µ —Å—Ö–æ–∂–µ—Å—Ç–∏


class SearchResponse(BaseModel):
    results: List[SearchResult]
    reformulated_query: str  # –£—Ç–æ—á–Ω–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ—Ç YandexGPT
    original_query: str  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è


class SimplifyRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ —É–ø—Ä–æ—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
    text: str
    max_length: Optional[int] = 300


class SimplifyResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —É–ø—Ä–æ—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞"""
    original_text: str
    simplified_text: str
    tokens_used: int


# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI
app = FastAPI(
    title="Normative Documents QA API",
    description="API –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –ñ–ö–•",
    version="1.0.0",
    default_response_class=JSONResponse
)


class NumpyEncoder(json.JSONEncoder):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π encoder –¥–ª—è numpy —Ç–∏–ø–æ–≤"""
    def default(self, obj):
        if hasattr(obj, 'item'):
            return obj.item()
        elif hasattr(obj, 'tolist'):
            return obj.tolist()
        return super().default(obj)


# –ö–∞—Å—Ç–æ–º–Ω—ã–π response handler
async def custom_response(request, response):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ JSON"""
    response.media_type = "application/json"
    response.body = json.dumps(
        response.body,
        cls=NumpyEncoder,
        ensure_ascii=False
    ).encode('utf-8')
    return response

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
vectorstores: List[FAISS] = []  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö 18 –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –±–∞–∑
model: Optional[SentenceTransformer] = None


@app.on_event("startup")
async def startup_event():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global vectorstores, model

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Backend API —Å–µ—Ä–≤–µ—Ä–∞...")

    if not VECTORDB_DIR.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {VECTORDB_DIR}")
        raise RuntimeError(f"Vector DB dir not found: {VECTORDB_DIR}")

    try:
        logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {EMBEDDING_MODEL}")
        model = SentenceTransformer(EMBEDDING_MODEL)
        embeddings = SentenceTransformerEmbeddings(model)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ 18 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –±–∞–∑
        db_dirs = sorted([d for d in VECTORDB_DIR.iterdir()
                         if d.is_dir()
                         and not d.name.startswith('.')
                         and not d.name.startswith('unified')])  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º unified

        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(db_dirs)} –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î")

        total_vectors = 0
        for i, db_dir in enumerate(db_dirs, 1):
            try:
                logger.info(f"[{i}/{len(db_dirs)}] –ó–∞–≥—Ä—É–∑–∫–∞: {db_dir.name}")
                vs = FAISS.load_local(str(db_dir), embeddings, allow_dangerous_deserialization=True)
                vectorstores.append(vs)
                count = vs.index.ntotal
                total_vectors += count
                logger.info(f"  ‚îú‚îÄ –í–µ–∫—Ç–æ—Ä–æ–≤: {count}")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {db_dir.name}: {e}")

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectorstores)} –ë–î, –≤—Å–µ–≥–æ {total_vectors} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        logger.info("‚úÖ Backend API —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        raise


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "service": "Normative Documents QA API",
        "status": "running",
        "version": "1.0.0"
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è"""
    return {
        "status": "healthy" if len(vectorstores) > 0 else "uninitialized",
        "model": EMBEDDING_MODEL,
        "vectordb": str(VECTORDB_DIR),
        "databases_loaded": len(vectorstores)
    }


@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
    if not vectorstore:
        raise HTTPException(status_code=503, detail="Vector database not loaded")

    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å FAISS
        index = vectorstore.index

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        chunks_count = index.ntotal

        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–ª–∏ —Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ—Ç–¥–µ–ª—å–Ω–æ
        # –ü—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± - –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        documents = set()
        # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø)
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å–ª—É—á–∞–µ –ª—É—á—à–µ —Ö—Ä–∞–Ω–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ë–î

        return {
            "documents_count": 18,  # –ò–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ —Å–æ–∑–¥–∞–Ω–∏—è –ë–î
            "chunks_count": chunks_count,
            "model": EMBEDDING_MODEL
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=f"Stats error: {str(e)}")


@app.get("/documents")
async def get_documents():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î"""
    if not vectorstore:
        raise HTTPException(status_code=503, detail="Vector database not loaded")

    try:
        # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–∑–¥–∞–Ω–∏—è –ë–î
        documents = [
            "–ñ–∏–ª–∏—â–Ω—ã–π –∫–æ–¥–µ–∫—Å –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ (—á–∞—Å—Ç—å –ø–µ—Ä–≤–∞—è)",
            "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ (—á–∞—Å—Ç—å –≤—Ç–æ—Ä–∞—è)",
            "–ü–ü –†–§ –æ—Ç 06.05.2011 ‚Ññ 354",
            "–ü–ü –†–§ –æ—Ç 13.08.2006 ‚Ññ 491",
            "–ü–ü –†–§ –æ—Ç 23.09.2010 ‚Ññ 731",
            "–ü–ü –†–§ –æ—Ç 16.04.2013 ‚Ññ 344",
            "–ü–ü –†–§ –æ—Ç 05.05.2011 ‚Ññ 355",
            "–ü–ü –†–§ –æ—Ç 14.02.2012 ‚Ññ 128",
            "–ü–ü –†–§ –æ—Ç 15.05.2013 ‚Ññ 416",
            "–ü–ü –†–§ –æ—Ç 06.02.2011 ‚Ññ 56",
            "–ü–ü –†–§ –æ—Ç 04.05.2012 ‚Ññ 439",
            "–ü–ü –†–§ –æ—Ç 13.08.2006 ‚Ññ 491",
            "–ü–ü –†–§ –æ—Ç 27.08.2016 ‚Ññ 857",
            "–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –†–§ –æ—Ç 26.09.2014 ‚Ññ 976",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ—Ç 27.07.2010 ‚Ññ 190-–§–ó",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ—Ç 21.12.2013 ‚Ññ 361-–§–ó",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ—Ç 31.12.2017 ‚Ññ 506-–§–ó"
        ]

        return {"documents": documents}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=f"Documents error: {str(e)}")


@app.post("/search", response_model=List[SearchResult])
async def search_documents(request: QueryRequest):
    """
    –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (18 –±–∞–∑)

    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    if len(vectorstores) == 0:
        raise HTTPException(status_code=503, detail="Vector databases not loaded")

    try:
        import time
        total_start = time.time()

        # –®–∞–≥ 1: –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ YandexGPT –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        reformulate_start = time.time()
        reformulated_query = await reformulate_query(request.query)
        reformulate_time = time.time() - reformulate_start
        logger.info(f"‚è±Ô∏è –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: {reformulate_time:.2f} —Å–µ–∫")

        # –®–∞–≥ 2: –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º 18 –±–∞–∑–∞–º —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Å–∫–æ—Ä–∞–º–∏
        all_results = []
        per_db_k = 5  # –ë–µ—Ä–µ–º —Ç–æ–ø-5 –∏–∑ –∫–∞–∂–¥–æ–π –±–∞–∑—ã (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)

        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ {len(vectorstores)} –±–∞–∑–∞–º (k={per_db_k} –∏–∑ –∫–∞–∂–¥–æ–π)")

        for i, vs in enumerate(vectorstores):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º similarity_search_with_score —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                docs_with_scores = vs.similarity_search_with_score(reformulated_query, k=per_db_k)
                for doc, score in docs_with_scores:
                    all_results.append((doc, float(score)))
            except KeyError as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞–∑—ã —Å –æ—à–∏–±–∫–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –≤ –±–∞–∑–µ {i}: {e}")
                continue
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ {i}: {e}")
                continue

        logger.info(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_results)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        search_time = time.time() - total_start
        logger.info(f"‚è±Ô∏è –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–∞–º: {search_time:.2f} —Å–µ–∫")

        # –®–∞–≥ 3: –ü–µ—Ä–≤–∏—á–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score
        all_results.sort(key=lambda x: x[1])

        # –®–∞–≥ 3.5: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        dedup_start = time.time()
        seen_contents = {}
        unique_results = []
        duplicates_count = 0

        for doc, score in all_results:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (–ø–µ—Ä–≤—ã–µ 500)
            content_key = doc.page_content[:500] if len(doc.page_content) > 500 else doc.page_content
            if content_key not in seen_contents:
                seen_contents[content_key] = True
                unique_results.append((doc, score))
            else:
                duplicates_count += 1

        dedup_time = time.time() - dedup_start
        if duplicates_count > 0:
            logger.info(f"üóëÔ∏è –£–±—Ä–∞–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count} (–∑–∞ {dedup_time:.3f} —Å–µ–∫)")
        else:
            logger.info(f"‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ—Ç (–∑–∞ {dedup_time:.3f} —Å–µ–∫)")

        # –ë–µ—Ä–µ–º —Ç–æ–ø-20 –¥–ª—è —Ä–µ—Ä–∞–Ω–∂–∏–Ω–≥–∞
        candidates_for_rerank = unique_results[:min(len(unique_results), 20)]

        # –®–∞–≥ 4: –†–µ—Ä–∞–Ω–∂–∏–Ω–≥ —á–µ—Ä–µ–∑ YandexGPT - –û–¢–ö–õ–Æ–ß–ï–ù–û (–ø–ª–æ—Ö–∞—è —Ä–∞–±–æ—Ç–∞)
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–æ—á–Ω–µ–µ —á–µ–º LLM-—Ä–µ—Ä–∞–Ω–∂–∏–Ω–≥–∞
        # logger.info(f"üîÑ –†–µ—Ä–∞–Ω–∂–∏–Ω–≥ {len(candidates_for_rerank)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ YandexGPT...")
        # results_with_scores = await rerank_results_with_llm(request.query, candidates_for_rerank)

        # –ë–µ—Ä–µ–º top_k –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        results_with_scores = candidates_for_rerank[:request.top_k]

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        formatted_results = []
        for doc, score in results_with_scores:
            try:
                metadata = doc.metadata
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã
                clean_metadata = {}
                for key, value in metadata.items():
                    if hasattr(value, 'item'):
                        clean_metadata[key] = value.item()
                    elif isinstance(value, (int, float, str, bool, list, dict, type(None))):
                        clean_metadata[key] = value
                    else:
                        clean_metadata[key] = str(value)

                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏–∑ L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è FAISS
                # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: cosine = 1 - (distance^2) / 2
                cosine_similarity = 1 - (float(score) ** 2) / 2
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
                similarity = max(0.0, min(1.0, cosine_similarity))
                formatted_results.append(SearchResult(
                    document=str(clean_metadata.get('document', 'Unknown')),
                    article=str(clean_metadata.get('article', '')),
                    content=str(doc.page_content),
                    metadata=clean_metadata,
                    similarity=round(similarity, 4)
                ))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                continue

        logger.info(f"üîç –ü–æ–∏—Å–∫: '{request.query[:50]}...' -> {len(formatted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

        # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç —Å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        response = SearchResponse(
            results=formatted_results,
            reformulated_query=reformulated_query,
            original_query=request.query
        )

        # –ó–∞–º–µ—Ä—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è
        total_time = time.time() - total_start
        logger.info(f"‚è±Ô∏è –û–ë–©–ï–ï –í–†–ï–ú–Ø: {total_time:.2f} —Å–µ–∫ (–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: {reformulate_time:.2f}s, –ø–æ–∏—Å–∫: {search_time:.2f}s, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {dedup_time:.3f}s)")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON
        return JSONResponse(
            content=response.model_dump()
        )

    except Exception as e:
        import traceback
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        logger.error(f"TRACEBACK:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

async def simplify_text_with_llm(text: str, max_length: int = 300) -> tuple[str, int]:
    """
    –£–ø—Ä–æ—â–∞–µ—Ç –∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
    Returns:
        (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π_—Ç–µ–∫—Å—Ç, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Ç–æ–∫–µ–Ω–æ–≤)
    """
    if not YANDEX_API_KEY or not YANDEX_FOLDER_ID:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç Yandex API –∫–ª—é—á–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç")
        return text, 0
    
    prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—é —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ñ–ö–•.

–ü–†–ê–í–ò–õ–ê:
1. –ó–∞–º–µ–Ω–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
2. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
3. –°–æ—Ö—Ä–∞–Ω—è–π –æ—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª
4. –ò–∑–±–µ–≥–∞–π –∫–∞–Ω—Ü–µ–ª—è—Äisms
5. –ü–∏—à–∏ —Ç–∞–∫ —á—Ç–æ–±—ã –ø–æ–Ω—è–ª –æ–±—ã—á–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ –±–µ–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
6. –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: –Ω–µ –±–æ–ª–µ–µ {max_length} —Å–ª–æ–≤

–ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢:
{text}

–£–ü–†–û–©–ï–ù–ù–´–ô –¢–ï–ö–°–¢:"""

    url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
    headers = {
        "Authorization": f"Api-Key {YANDEX_API_KEY}",
        "Content-Type": "application/json"
    }
    
    body = {
        "modelUri": f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-lite",
        "completionOptions": {
            "stream": False,
            "temperature": 0.5,
            "maxTokens": max_length
        },
        "messages": [
            {
                "role": "user",
                "text": prompt
            }
        ]
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.post(url, headers=headers, json=body)
            response.raise_for_status()
            result = response.json()
            
            simplified_text = result["choices"][0]["message"]["text"].strip()
            tokens_used = result.get("usage", {}).get("completionTokens", 0)
            
            logger.info(f"‚úÖ –¢–µ–∫—Å—Ç —É–ø—Ä–æ—â–µ–Ω: {len(text)} -> {len(simplified_text)} —Å–∏–º–≤–æ–ª–æ–≤, —Ç–æ–∫–µ–Ω–æ–≤: {tokens_used}")
            return simplified_text, tokens_used
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–ø—Ä–æ—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return text, 0



@app.post("/simplify", response_model=SimplifyResult)
async def simplify_text(request: SimplifyRequest):
    """
    –£–ø—Ä–æ—â–∞–µ—Ç –∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π —è–∑—ã–∫
    
    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
        
    Returns:
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö
    """
    try:
        simplified_text, tokens_used = await simplify_text_with_llm(
            request.text, 
            request.max_length
        )
        
        return SimplifyResult(
            original_text=request.text[:200] + "..." if len(request.text) > 200 else request.text,
            simplified_text=simplified_text,
            tokens_used=tokens_used
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ endpoint simplify: {e}")
        raise HTTPException(status_code=500, detail=f"Simplification error: {str(e)}")




def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
    import uvicorn
    
    logger.info("=" * 80)
    logger.info("–ó–ê–ü–£–°–ö BACKEND API –°–ï–†–í–ï–†–ê")
    logger.info("=" * 80)
    logger.info(f"üìç API endpoint: http://0.0.0.0:8001")
    logger.info(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://0.0.0.0:8001/docs")
    logger.info("=" * 80)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        log_level="info"
    )


if __name__ == "__main__":
    main()
