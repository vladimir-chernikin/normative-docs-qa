#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.embeddings import Embeddings
from typing import List

class SentenceTransformerEmbeddings(Embeddings):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è SentenceTransformer —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å LangChain"""

    def __init__(self, model: SentenceTransformer):
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, show_progress_bar=False).tolist()

    def embed_query(self, text: str) -> List[float]:
        return self.model.encode([text], show_progress_bar=False)[0].tolist()


def test_search():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ 196"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    st_model = SentenceTransformer("cointegrated/rubert-tiny2")
    embeddings = SentenceTransformerEmbeddings(st_model)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã...")
    vector_store = FAISS.load_local(
        "vectordb/rubert-tiny2-faiss",
        embeddings,
        allow_dangerous_deserialization=True
    )

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    queries = [
        "–ö–∞–∫–æ–π —Å—Ä–æ–∫ –∏—Å–∫–æ–≤–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏?",
        "–°—Ç–∞—Ç—å—è 196 –ì–ö –†–§",
        "—Å—Ä–æ–∫ –¥–∞–≤–Ω–æ—Å—Ç–∏ –ø–æ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–º –ø–ª–∞—Ç–µ–∂–∞–º",
        "–æ–±—â–∏–π —Å—Ä–æ–∫ –∏—Å–∫–æ–≤–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç"
    ]

    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ –ü–û–ò–°–ö–ê –í –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–ï")
    print("=" * 60)

    for query in queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: {query}")
        print("-" * 60)

        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        results = vector_store.similarity_search_with_score(query, k=3)

        for i, (doc, score) in enumerate(results, 1):
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º score (L2 distance) –≤ –ø—Ä–æ—Ü–µ–Ω—Ç
            similarity = 1.0 / (1.0 + float(score))
            relevance = min(99.99, similarity * 100)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ç–∞—Ç—å—é 196
            is_art196 = "–°—Ç–∞—Ç—å—è 196" in doc.page_content or "196" in doc.page_content

            marker = "‚úÖ" if is_art196 else "  "
            print(f"{marker} –†–µ–∑—É–ª—å—Ç–∞—Ç #{i} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.1f}%)")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source_file', 'N/A')}")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç—Ä—ã–≤–æ–∫ —Ç–µ–∫—Å—Ç–∞
            content_preview = doc.page_content[:200].replace('\n', ' ')
            print(f"   –¢–µ–∫—Å—Ç: {content_preview}...")

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å—Ç–∞—Ç—å—é 196 - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
            if is_art196:
                print("\n   üéâ –ù–ê–ô–î–ï–ù–ê –°–¢–ê–¢–¨–Ø 196!")
                # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫—É —Å–æ —Å—Ç–∞—Ç—å–µ–π
                lines = doc.page_content.split('\n')
                for line in lines:
                    if '196' in line or ('–∏—Å–∫–æ–≤–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏' in line and '—Å—Ä–æ–∫' in line):
                        print(f"   üìå {line.strip()}")

        print()

    print("=" * 60)
    print("–¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 60)


if __name__ == '__main__':
    test_search()
