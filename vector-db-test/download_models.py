#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É.
–í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
–í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –¢–û–õ–¨–ö–û —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.
"""
import os
import json
import shutil
from pathlib import Path
import torch
from transformers import AutoModel, AutoTokenizer

project_root = Path(__file__).parent
config_path = project_root / "config" / "config.json"
models_dir = project_root / "local_models"

# –ú–∞–ø–ø–∏–Ω–≥ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–º–µ–Ω –Ω–∞ –ø–æ–ª–Ω—ã–µ –ø—É—Ç–∏ –≤ Hugging Face
MODEL_MAPPING = {
    "rubert-tiny2": "cointegrated/rubert-tiny2",
    "multilingual-e5-small": "intfloat/multilingual-e5-small",
    "paraphrase-multilingual-MiniLM-L12-v2": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "labse": "sentence-transformers/LaBSE",
    "frida": "ai-forever/FRIDA"
    # "openai-e5-large": "text-embedding-3-large"  # OpenAI –º–æ–¥–µ–ª—å - –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏–π API
}

def verify_model_integrity(model_path: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–∏
        
    Returns:
        True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ü–µ–ª–æ—Å—Ç–Ω–∞—è, –∏–Ω–∞—á–µ False
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
    required_files = ["config.json"]
    model_files = ["model.safetensors", "pytorch_model.bin"]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    for file in required_files:
        if not (model_path / file).exists():
            return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ (—Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
    has_model_file = any((model_path / file).exists() for file in model_files)
    if not has_model_file:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    tokenizer_files = ["tokenizer.json", "vocab.txt"]
    has_tokenizer = any((model_path / file).exists() for file in tokenizer_files)
    if not has_tokenizer:
        return False
        
    return True

def download_models():
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏, –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤ config.json."""
    print("\n" + "="*80)
    print("üöÄ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –î–õ–Ø –ê–í–¢–û–ù–û–ú–ù–û–ô –†–ê–ë–û–¢–´")
    print("="*80)
    print("‚ö†Ô∏è  –í–ê–ñ–ù–û: –≠—Ç–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.")
    print("‚ö†Ô∏è  –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ.")
    print("‚ö†Ô∏è  –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª–∏, –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞.")
    print("="*80 + "\n")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config.json: {e}")
        return

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    models = config.get("models", [])
    if not models:
        print("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤ config.json –ø—É—Å—Ç. –ù–µ—á–µ–≥–æ —Å–∫–∞—á–∏–≤–∞—Ç—å.")
        return

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {len(models)}")
    models_dir.mkdir(exist_ok=True)

    for model_config in models:
        short_name = model_config.get("name")
        model_type = model_config.get("type")
        full_model_path = model_config.get("model_path")
        
        if not short_name or not full_model_path:
            print(f"‚ö†Ô∏è –ù–µ–ø–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
            continue

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª–∏ OpenAI, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        if model_type == "openai":
            # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏–π API
            """
            print(f"‚ÑπÔ∏è –ú–æ–¥–µ–ª—å '{short_name}' –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.")
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è OpenAI –º–æ–¥–µ–ª–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–º —Ñ–∞–π–ª–æ–º
            save_path = models_dir / short_name
            save_path.mkdir(exist_ok=True, parents=True)
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è OpenAI –º–æ–¥–µ–ª–∏
            config_file = save_path / "config.json"
            openai_config = {
                "model_name": full_model_path,
                "type": "openai",
                "requires_api_key": True,
                "embedding_size": model_config.get("embedding_size", 3072)
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(openai_config, f, indent=2)
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
            info_file = save_path / "openai_model_info.txt"
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write(f"OpenAI Model: {full_model_path}\n")
                f.write("This is a placeholder for an OpenAI API model.\n")
                f.write("To use this model, set the OPENAI_API_KEY environment variable.\n")
                f.write("Example: export OPENAI_API_KEY='your-api-key'\n")
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ä–∫–µ—Ä —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            marker_file = save_path / ".download_complete"
            with open(marker_file, 'w') as f:
                f.write(f"OpenAI API Model: {full_model_path}\n")
                f.write(f"Date: {__import__('datetime').datetime.now().isoformat()}\n")
                f.write(f"REQUIRES API KEY: This model requires an OpenAI API key to function\n")
            
            print(f"‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ '{short_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ.")
            """
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{short_name}' –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI API –∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
            continue

        save_path = models_dir / short_name # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É —Å –∫–æ—Ä–æ—Ç–∫–∏–º –∏–º–µ–Ω–µ–º
        marker_file = save_path / ".download_complete"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –∏ —Ü–µ–ª–æ—Å—Ç–Ω–∞ –ª–∏ –æ–Ω–∞
        if save_path.exists() and marker_file.exists() and verify_model_integrity(save_path):
            print(f"‚úÖ –ú–æ–¥–µ–ª—å '{short_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ {save_path} –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
            continue
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —É–¥–∞–ª—è–µ–º –µ—ë
        if save_path.exists() and (not marker_file.exists() or not verify_model_integrity(save_path)):
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{short_name}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞. –£–¥–∞–ª—è—é –∏ —Å–∫–∞—á–∏–≤–∞—é –∑–∞–Ω–æ–≤–æ.")
            shutil.rmtree(save_path, ignore_errors=True)

        print(f"‚è≥ –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å '{full_model_path}' (–∫–∞–∫ '{short_name}') –≤ {save_path}...")
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            save_path.mkdir(exist_ok=True, parents=True)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if model_type == "sentence-transformers":
                # –î–ª—è sentence-transformers –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoModel –∏ AutoTokenizer –Ω–∞–ø—Ä—è–º—É—é
                print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ sentence-transformers: {full_model_path}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                tokenizer = AutoTokenizer.from_pretrained(full_model_path)
                model = AutoModel.from_pretrained(full_model_path)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                model.save_pretrained(str(save_path))
                tokenizer.save_pretrained(str(save_path))
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")
                
            elif model_type == "transformers":
                # –î–ª—è –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ transformers –∏—Å–ø–æ–ª—å–∑—É–µ–º AutoModel –∏ AutoTokenizer
                print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ transformers: {full_model_path}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º accelerate
                try:
                    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å device_map='auto'...")
                    model = AutoModel.from_pretrained(full_model_path, device_map='auto')
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å device_map: {e}")
                    print("üîÑ –ü—Ä–æ–±—É—é –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ device_map...")
                    model = AutoModel.from_pretrained(full_model_path)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                tokenizer = AutoTokenizer.from_pretrained(full_model_path)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
                model.save_pretrained(str(save_path))
                tokenizer.save_pretrained(str(save_path))
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")
            else:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
                continue
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ä–∫–µ—Ä —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            with open(marker_file, 'w') as f:
                f.write(f"Downloaded from: {full_model_path}\n")
                f.write(f"Date: {__import__('datetime').datetime.now().isoformat()}\n")
                f.write(f"AUTONOMOUS MODE: This model is used in offline mode only\n")
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å '{short_name}' —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ.")
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å '{short_name}': {e}")
            # –£–¥–∞–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if save_path.exists():
                shutil.rmtree(save_path, ignore_errors=True)

    print("\n" + "="*80)
    print("üéâ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*80)
    print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ.")
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ.")
    print("‚úÖ –î–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã –≤—ã–ø–æ–ª–Ω–∏—Ç–µ: ./start_with_models.sh")
    # print("‚ö†Ô∏è –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è OpenAI –º–æ–¥–µ–ª–µ–π —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY") # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ
    print("="*80)

if __name__ == "__main__":
    download_models()