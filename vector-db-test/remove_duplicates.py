#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
"""

import sys
import os
from pathlib import Path
import json
import logging
import pickle

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List, Dict, Set
from utils.embeddings import SentenceTransformerEmbeddings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def remove_duplicates():
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""

    logger.info("=" * 80)
    logger.info("–£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í –ò–ó –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ô –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´")
    logger.info("=" * 80)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    vectordb_dir = project_root / "vector-db-test/vectordb"
    model_name = "intfloat/multilingual-e5-base"
    input_dir = vectordb_dir / "unified_all_docs_e5-base"
    output_dir = vectordb_dir / "unified_all_docs_e5-base_dedup"

    if not input_dir.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
        return False

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    model = SentenceTransformer(model_name)
    embeddings = SentenceTransformerEmbeddings(model)
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –ë–î
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –ë–î –∏–∑: {input_dir}")
    vector_store = FAISS.load_local(
        str(input_dir),
        embeddings,
        allow_dangerous_deserialization=True
    )

    original_vectors = vector_store.index.ntotal
    logger.info(f"üìä –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {original_vectors}")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    logger.info("üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    all_docs = []
    seen_content: Set[str] = set()

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ docstore
    for doc_id in vector_store.index_to_docstore_id.values():
        try:
            doc = vector_store.docstore.search(doc_id)
            if doc and hasattr(doc, 'page_content'):
                content = doc.page_content

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                if content not in seen_content:
                    seen_content.add(content)
                    all_docs.append(doc)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
            continue

    unique_docs = len(all_docs)
    duplicates = original_vectors - unique_docs

    logger.info("=" * 80)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò")
    logger.info("=" * 80)
    logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {original_vectors}")
    logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {unique_docs}")
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates} ({duplicates/original_vectors*100:.1f}%)")

    if duplicates == 0:
        logger.info("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        return True

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –ë–î –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    logger.info("\n" + "=" * 80)
    logger.info("–°–û–ó–î–ê–ù–ò–ï –û–ß–ò–©–ï–ù–ù–û–ô –ë–ê–ó–´")
    logger.info("=" * 80)

    logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π FAISS –±–∞–∑—ã –∏–∑ {unique_docs} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –ë–î –∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    cleaned_store = FAISS.from_documents(
        all_docs,
        embeddings,
    )

    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –±–∞–∑–∞: {cleaned_store.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—É—é –ë–î
    logger.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_dir}")
    output_dir.mkdir(parents=True, exist_ok=True)

    cleaned_store.save_local(str(output_dir))
    logger.info("‚úÖ –û—á–∏—â–µ–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    report = {
        "cleaned_db_path": str(output_dir),
        "original_db_path": str(input_dir),
        "model": model_name,
        "original_vectors": int(original_vectors),
        "unique_vectors": int(cleaned_store.index.ntotal),
        "duplicates_removed": int(duplicates),
        "dimension": int(cleaned_store.index.d),
        "created_at": str(Path.cwd())
    }

    report_file = project_root / "vector-db-test/dedup_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    logger.info(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    logger.info("=" * 80)
    logger.info(f"üìÅ –ü—É—Ç—å –∫ –æ—á–∏—â–µ–Ω–Ω–æ–π –ë–î: {output_dir}")
    logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {cleaned_store.index.ntotal}")
    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates}")

    return True


if __name__ == '__main__':
    success = remove_duplicates()
    sys.exit(0 if success else 1)
