#!/usr/bin/env python3
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö embedding –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ embedding –º–æ–¥–µ–ª–∏ (sentence-transformers/transformers)
2. –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏—Ö —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
3. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
4. –ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
"""

import os
import sys
import json
import time
import logging
import threading
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer
import pynvml
import psutil

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ config –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
sys.path.append(str(Path(__file__).parent.parent))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('embedding_models.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class GPUMonitor:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU –ø–∞–º—è—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    def __init__(self):
        try:
            pynvml.nvmlInit()
            self.gpu_available = True
            self.device_count = pynvml.nvmlDeviceGetCount()
        except Exception as e:
            logger.warning(f"GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.gpu_available = False
            self.device_count = 0
        
        self.monitoring = False
        self.thread = None
        
    def start_monitoring(self, device_id: int = 0):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU"""
        if not self.gpu_available:
            logger.warning("GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return
            
        self.monitoring = True
        self.thread = threading.Thread(target=self._monitor_loop, args=(device_id,))
        self.thread.daemon = True
        self.thread.start()
        logger.info("GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—É—â–µ–Ω")
        
    def _monitor_loop(self, device_id: int):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        while self.monitoring:
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                used_mb = mem_info.used // 1024**2
                total_mb = mem_info.total // 1024**2
                usage_percent = (used_mb / total_mb) * 100
                
                print(f"\r[GPU-{device_id}] {used_mb:,}/{total_mb:,} MB ({usage_percent:.1f}%)", 
                      end="", flush=True)
                time.sleep(0.5)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU: {e}")
                break
                
    def stop_monitoring(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU"""
        self.monitoring = False
        if self.thread:
            self.thread.join(timeout=2)
        print()  # –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
        logger.info("GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
    def get_gpu_info(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU"""
        if not self.gpu_available:
            return {"available": False}
            
        gpu_info = {"available": True, "devices": []}
        
        for i in range(self.device_count):
            try:
                handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                
                gpu_info["devices"].append({
                    "id": i,
                    "name": name,
                    "memory_total_mb": mem_info.total // 1024**2,
                    "memory_used_mb": mem_info.used // 1024**2,
                    "memory_free_mb": mem_info.free // 1024**2
                })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU {i}: {e}")
                
        return gpu_info


class EmbeddingModelLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è embedding –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self, config_path: str = "config/config.json"):
        self.config_path = Path(config_path)
        self.models = {}
        self.gpu_monitor = GPUMonitor()
        self.device = self._detect_device()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
        self.model_configs = self._load_model_configs()
        
    def _detect_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        if torch.cuda.is_available():
            device = "cuda"
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω CUDA: {torch.cuda.get_device_name()}")
        else:
            device = "cpu"
            logger.info("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return device
        
    def _load_model_configs(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            models = config.get('models', [])
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π")
            return models
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return []
    
    def get_model_config_by_name(self, model_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏"""
        configs = self._load_model_configs()
        for config in configs:
            if config.get('name') == model_name:
                return config
        return None
    
    def load_model_by_name(self, model_name: str) -> Optional[Union[SentenceTransformer, Tuple]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –∏–º–µ–Ω–∏"""
        config = self.get_model_config_by_name(model_name)
        if config is None:
            logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
        return self.load_model(config)
            
    def load_model(self, model_config: Dict) -> Optional[Union[SentenceTransformer, Tuple]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            model_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        model_name = model_config['name']
        model_path = model_config['model_path']
        model_type = model_config['type']
        estimated_vram = model_config.get('estimated_vram_mb', 0)
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name} ({model_type})")
        logger.info(f"–ü—É—Ç—å: {model_path}")
        logger.info(f"–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM: {estimated_vram} MB")
        
        start_time = time.time()
        
        try:
            if model_type == "sentence-transformers":
                model = SentenceTransformer(model_path, device=self.device)
                
            elif model_type == "transformers":
                if self.device == "cuda":
                    try:
                        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å accelerate
                        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º accelerate (device_map='auto')")
                        model = AutoModel.from_pretrained(model_path, device_map='auto')
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å accelerate: {e}")
                        logger.info("–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ device_map")
                        model = AutoModel.from_pretrained(model_path).to(self.device)
                else:
                    model = AutoModel.from_pretrained(model_path)
                    
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = (model, tokenizer)
                
            else:
                logger.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
                return None
                
            load_time = time.time() - start_time
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫")
            
            self.models[model_name] = {
                'model': model,
                'config': model_config,
                'load_time': load_time
            }
            
            return model
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return None
            
    def test_model_embedding(self, model_name: str, test_texts: List[str]) -> Dict:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç embedding –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            test_texts: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if model_name not in self.models:
            logger.error(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return {}
            
        model_info = self.models[model_name]
        model = model_info['model']
        config = model_info['config']
        
        logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name} –Ω–∞ {len(test_texts)} —Ç–µ–∫—Å—Ç–∞—Ö")
        
        results = {
            'model_name': model_name,
            'model_type': config['type'],
            'embedding_size': config['embedding_size'],
            'test_count': len(test_texts),
            'embeddings': [],
            'processing_times': [],
            'average_time': 0,
            'total_time': 0
        }
        
        total_start = time.time()
        
        for i, text in enumerate(test_texts):
            start_time = time.time()
            
            try:
                if config['type'] == "sentence-transformers":
                    embedding = model.encode([text])[0]
                    
                elif config['type'] == "transformers":
                    model_obj, tokenizer = model
                    inputs = tokenizer(text, return_tensors='pt', 
                                     truncation=True, padding=True)
                    
                    if self.device == "cuda":
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                        
                    with torch.no_grad():
                        outputs = model_obj(**inputs)
                        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]
                        
                processing_time = time.time() - start_time
                results['processing_times'].append(processing_time)
                results['embeddings'].append(embedding.tolist())
                
                logger.info(f"–¢–µ–∫—Å—Ç {i+1}: {processing_time:.3f}—Å, —Ä–∞–∑–º–µ—Ä: {len(embedding)}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ {i+1}: {e}")
                results['processing_times'].append(-1)
                results['embeddings'].append([])
                
        results['total_time'] = time.time() - total_start
        results['average_time'] = np.mean([t for t in results['processing_times'] if t > 0])
        
        logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {results['total_time']:.2f}—Å")
        logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–µ–∫—Å—Ç: {results['average_time']:.3f}—Å")
        
        return results
        
    def load_and_test_all_models(self, test_texts: List[str]) -> Dict:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            test_texts: –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            –°–≤–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
        gpu_info = self.gpu_monitor.get_gpu_info()
        logger.info(f"GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {json.dumps(gpu_info, indent=2, ensure_ascii=False)}")
        
        results = {
            'system_info': {
                'device': self.device,
                'gpu_info': gpu_info,
                'cpu_count': psutil.cpu_count(),
                'memory_total_gb': psutil.virtual_memory().total / (1024**3)
            },
            'models_tested': [],
            'test_texts': test_texts,
            'summary': {}
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º)
        sorted_models = sorted(self.model_configs, key=lambda x: x['priority'])
        
        for model_config in sorted_models:
            model_name = model_config['name']
            
            logger.info(f"\n{'='*60}")
            logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
            logger.info(f"{'='*60}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º GPU –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            if self.device == "cuda":
                self.gpu_monitor.start_monitoring()
                
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model = self.load_model(model_config)
            
            if model is None:
                logger.error(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª—å {model_name} - –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
                continue
                
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
            test_results = self.test_model_embedding(model_name, test_texts)
            results['models_tested'].append(test_results)
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
            if self.device == "cuda":
                self.gpu_monitor.stop_monitoring()
                
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
            del self.models[model_name]
            if self.device == "cuda":
                torch.cuda.empty_cache()
                
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏ –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É
        if results['models_tested']:
            summary = {}
            for test_result in results['models_tested']:
                summary[test_result['model_name']] = {
                    'avg_time': test_result['average_time'],
                    'total_time': test_result['total_time'],
                    'embedding_size': test_result['embedding_size'],
                    'model_type': test_result['model_type']
                }
            results['summary'] = summary
            
        return results
        
    def save_results(self, results: Dict, output_file: str = "model_test_results.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π
    test_texts = [
        "–°—Ç–∞—Ç—å—è 1. –û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
        "–ù–∞—Å—Ç–æ—è—â–∏–π –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –≤–æ–∑–Ω–∏–∫–∞—é—â–∏–µ –≤ —Å—Ñ–µ—Ä–µ –∂–∏–ª–∏—â–Ω–æ–≥–æ –ø—Ä–∞–≤–∞",
        "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è",
        "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Å –º–æ–º–µ–Ω—Ç–∞ –µ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏",
        "–î–æ–≥–æ–≤–æ—Ä —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞–∫–ª—é—á–µ–Ω–Ω—ã–º, –µ—Å–ª–∏ –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –≤ —Ç—Ä–µ–±—É–µ–º–æ–π –≤ –ø–æ–¥–ª–µ–∂–∞—â–∏—Ö —Å–ª—É—á–∞—è—Ö —Ñ–æ—Ä–º–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –¥–æ–≥–æ–≤–æ—Ä–∞"
    ]
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è embedding –º–æ–¥–µ–ª–µ–π")
    print(f"üìã –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(test_texts)}")
    print("="*70)
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–µ–π
    loader = EmbeddingModelLoader()
    
    if not loader.model_configs:
        logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ config/config.json")
        return
        
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    results = loader.load_and_test_all_models(test_texts)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    loader.save_results(results, "vector-db-test/scripts/model_test_results.json")
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print("\n" + "="*70)
    print("üìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*70)
    
    if results['summary']:
        for model_name, stats in results['summary'].items():
            print(f"\nüîπ {model_name}:")
            print(f"   ‚Ä¢ –¢–∏–ø –º–æ–¥–µ–ª–∏: {stats['model_type']}")
            print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä embedding: {stats['embedding_size']}")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–µ–∫—Å—Ç: {stats['avg_time']:.3f}—Å")
            print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {stats['total_time']:.2f}—Å")
    else:
        print("‚ùå –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞")
        
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main() 