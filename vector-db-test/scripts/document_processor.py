"""
–ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Ä–µ–∞–ª–∏–∑—É–µ—Ç –≠—Ç–∞–ø 2: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Markdown –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å MarkdownHeaderTextSplitter
- –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict

from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter, CharacterTextSplitter

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    content: str
    source_file: str
    document_path: str
    hierarchy_path: str
    header_1: Optional[str] = None
    header_2: Optional[str] = None 
    header_3: Optional[str] = None
    header_4: Optional[str] = None
    chunk_number: int = 1
    total_chunks_in_section: int = 1
    text_length: int = 0
    split_method: str = "MarkdownHeaderTextSplitter"
    chunk_id: str = ""

    def __post_init__(self):
        self.text_length = len(self.content)
        if not self.chunk_id:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —á–∞–Ω–∫–∞
            base_name = Path(self.source_file).stem.replace(" ", "_").replace("(", "").replace(")", "")
            self.chunk_id = f"{base_name}_chunk{self.chunk_number}"


class DocumentProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, config_path: str = "config/config.json"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        self.config = self._load_config(config_path)
        self.setup_splitters()
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "document_processing": {
                "chunk_size": 1000,
                "chunk_overlap": 200,
                "headers_to_split_on": [
                    ["#", "Header 1"],
                    ["##", "Header 2"],
                    ["###", "Header 3"],
                    ["####", "Header 4"]
                ]
            }
        }
    
    def setup_splitters(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π —Ç–µ–∫—Å—Ç–∞"""
        config = self.config.get("document_processing", {})
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        self.chunk_size = config.get("chunk_size", 1000)
        self.chunk_overlap = config.get("chunk_overlap", 200)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
        self.headers_to_split_on = config.get("headers_to_split_on", [
            ["#", "Header 1"],
            ["##", "Header 2"],
            ["###", "Header 3"],
            ["####", "Header 4"]
        ])
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=self.headers_to_split_on
        )
        
        self.recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        
        self.character_splitter = CharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        
        logger.info(f"–ù–∞—Å—Ç—Ä–æ–µ–Ω—ã —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}")
    
    def _clean_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤
        lines = text.split('\n')
        cleaned_text = ""
        for i, line in enumerate(lines):
            if i > 0 and line and line[0].islower() and not line[0].isspace():
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã –∏ –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞, 
                # –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ç—Ä–æ–∫–∏
                cleaned_text = cleaned_text.rstrip() + line
            else:
                if cleaned_text:
                    cleaned_text += '\n'
                cleaned_text += line
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        cleaned_lines = [line.strip() for line in cleaned_text.split('\n')]
        return '\n'.join(cleaned_lines)
    
    def analyze_document_structure(self, file_path: str) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MarkdownHeaderTextSplitter
        
        Args:
            file_path: –ü—É—Ç—å –∫ markdown —Ñ–∞–π–ª—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
            content = self._clean_text(content)
            
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            header_docs = self.markdown_splitter.split_text(content)
            
            logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ {os.path.basename(file_path)}: "
                      f"{len(header_docs)} —Å–µ–∫—Ü–∏–π")
            
            # –û–±–æ–≥–∞—â–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            enriched_docs = []
            for doc in header_docs:
                metadata = doc.metadata.copy()
                metadata.update({
                    'source_file': os.path.basename(file_path),
                    'document_path': file_path,
                    'text_length': len(doc.page_content),
                    'split_method': 'MarkdownHeaderTextSplitter'
                })
                
                enriched_docs.append({
                    'content': doc.page_content,
                    'metadata': metadata
                })
            
            return enriched_docs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {e}")
            return []
    
    def create_intelligent_chunks(self, documents):
        """
        –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –º–µ—Ç–æ–¥–∞ create_intelligent_chunks
        
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ RecursiveCharacterTextSplitter —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏.
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ DocumentChunk
        """
        all_chunks = []
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä —Å —Ä–∞–∑–º–µ—Ä–æ–º —á–∞–Ω–∫–∞ 1500 –∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º 150
        optimized_splitter = RecursiveCharacterTextSplitter(
            # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫,
            # –∑–∞—Ç–µ–º –ø–æ —Ç–æ—á–∫–∞–º —Å –ø—Ä–æ–±–µ–ª–æ–º, –∑–∞—Ç–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º, –∏ —Ç.–¥.
            separators=["\n\n", "\n", ". ", ", ", " ", ""],
            chunk_size=1500,
            chunk_overlap=150
        )
        
        for doc_idx, doc in enumerate(documents):
            content = doc['content']
            metadata = doc['metadata']
            content_length = len(content)
            
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–∫—Ü–∏–∏ {doc_idx + 1}, –¥–ª–∏–Ω–∞: {content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –≤—Å–µ—Ö —Å–µ–∫—Ü–∏–π
            chunks = optimized_splitter.split_text(content)
            split_method = "MarkdownHeaderTextSplitter+RecursiveCharacterTextSplitter"
            logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω RecursiveCharacterTextSplitter: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã DocumentChunk –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            section_chunks = []
            for chunk_idx, chunk_content in enumerate(chunks):
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
                clean_chunk_content = self._clean_text(chunk_content)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏
                hierarchy_parts = []
                for level in ["Header 1", "Header 2", "Header 3", "Header 4"]:
                    if level in metadata and metadata[level]:
                        hierarchy_parts.append(metadata[level])
                
                hierarchy_path = " > ".join(hierarchy_parts) if hierarchy_parts else "ROOT"
                
                # –°–æ–∑–¥–∞–µ–º DocumentChunk
                doc_chunk = DocumentChunk(
                    content=clean_chunk_content,
                    source_file=metadata.get('source_file', ''),
                    document_path=metadata.get('document_path', ''),
                    hierarchy_path=hierarchy_path,
                    header_1=metadata.get('Header 1'),
                    header_2=metadata.get('Header 2'),
                    header_3=metadata.get('Header 3'),
                    header_4=metadata.get('Header 4'),
                    chunk_number=chunk_idx + 1,
                    total_chunks_in_section=len(chunks),
                    split_method=split_method
                )
                
                section_chunks.append(doc_chunk)
            
            all_chunks.extend(section_chunks)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} –∏—Ç–æ–≥–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
        return all_chunks
    
    def create_zayavki_chunks(self, documents):
        """
        –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö zayavki (Q&A —Ñ–æ—Ä–º–∞—Ç)
        
        –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ç—Ä–µ–º—è —É—Ä–æ–≤–Ω—è–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤,
        –∑–∞—Ç–µ–º —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏—Ö –Ω–∞ —á–∞–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º 150.
        –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç—Å—è - –æ—Å—Ç–∞–µ—Ç—Å—è –º–∞–ª–µ–Ω—å–∫–∏–º —á–∞–Ω–∫–æ–º.
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã MarkdownHeaderTextSplitter)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ DocumentChunk
        """
        logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –±–∞–∑—ã zayavki")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º —Ç—Ä–µ–º —É—Ä–æ–≤–Ω—è–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        grouped_docs = {}
        
        for doc in documents:
            metadata = doc['metadata']
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∏–∑ —Ç—Ä–µ—Ö —É—Ä–æ–≤–Ω–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            header_key = (
                metadata.get('Header 1', ''),
                metadata.get('Header 2', ''),
                metadata.get('Header 3', '')
            )
            
            if header_key not in grouped_docs:
                grouped_docs[header_key] = []
            grouped_docs[header_key].append(doc)
        
        logger.info(f"üìä –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–æ –≤ {len(grouped_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø
        zayavki_splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", ". ", ", ", " ", ""],
            chunk_size=1500,
            chunk_overlap=150
        )
        
        all_chunks = []
        
        for header_key, docs_group in grouped_docs.items():
            header_1, header_2, header_3 = header_key
            
            if len(docs_group) == 1:
                # –û–¥–∏–Ω–æ—á–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç - —Å–æ–∑–¥–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π —á–∞–Ω–∫ –∫–∞–∫ –µ—Å—Ç—å
                doc = docs_group[0]
                logger.info(f"üìÑ –û–¥–∏–Ω–æ—á–Ω—ã–π Q&A: {header_1}/{header_2}/{header_3} - {len(doc['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏
                hierarchy_parts = [h for h in [header_1, header_2, header_3] if h]
                hierarchy_path = " > ".join(hierarchy_parts) if hierarchy_parts else "ROOT"
                
                # –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —á–∞–Ω–∫
                doc_chunk = DocumentChunk(
                    content=self._clean_text(doc['content']),
                    source_file=doc['metadata'].get('source_file', ''),
                    document_path=doc['metadata'].get('document_path', ''),
                    hierarchy_path=hierarchy_path,
                    header_1=header_1 if header_1 else None,
                    header_2=header_2 if header_2 else None,
                    header_3=header_3 if header_3 else None,
                    header_4=doc['metadata'].get('Header 4'),
                    chunk_number=1,
                    total_chunks_in_section=1,
                    split_method="MarkdownHeaderTextSplitter+ZayavkiSingle"
                )
                all_chunks.append(doc_chunk)
                
            else:
                # –ù–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ - –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º
                aggregated_content = "\n\n".join([doc['content'] for doc in docs_group])
                logger.info(f"üì¶ –ê–≥—Ä–µ–≥–∞—Ü–∏—è {len(docs_group)} Q&A: {header_1}/{header_2}/{header_3} - {len(aggregated_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
                chunks = zayavki_splitter.split_text(aggregated_content)
                logger.info(f"   ‚Üí –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ
                base_metadata = docs_group[0]['metadata']
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—É—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏
                hierarchy_parts = [h for h in [header_1, header_2, header_3] if h]
                hierarchy_path = " > ".join(hierarchy_parts) if hierarchy_parts else "ROOT"
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã
                for chunk_idx, chunk_content in enumerate(chunks):
                    doc_chunk = DocumentChunk(
                        content=self._clean_text(chunk_content),
                        source_file=base_metadata.get('source_file', ''),
                        document_path=base_metadata.get('document_path', ''),
                        hierarchy_path=hierarchy_path,
                        header_1=header_1 if header_1 else None,
                        header_2=header_2 if header_2 else None,
                        header_3=header_3 if header_3 else None,
                        header_4=base_metadata.get('Header 4'),
                        chunk_number=chunk_idx + 1,
                        total_chunks_in_section=len(chunks),
                        split_method="MarkdownHeaderTextSplitter+ZayavkiAggregated"
                    )
                    all_chunks.append(doc_chunk)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} –∏—Ç–æ–≥–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –±–∞–∑—ã zayavki")
        return all_chunks
    
    def process_document(self, file_path: str) -> List[DocumentChunk]:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            file_path: –ü—É—Ç—å –∫ markdown —Ñ–∞–π–ª—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {os.path.basename(file_path)}")
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        documents = self.analyze_document_structure(file_path)
        if not documents:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {file_path}")
            return []
        
        # –®–∞–≥ 2: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        chunks = self.create_intelligent_chunks(documents)
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def process_zayavki_document(self, file_path: str) -> List[DocumentChunk]:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ zayavki (Q&A).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º.
        
        Args:
            file_path: –ü—É—Ç—å –∫ markdown —Ñ–∞–π–ª—É –±–∞–∑—ã zayavki
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        """
        logger.info(f"üéØ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ zayavki: {os.path.basename(file_path)}")
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        documents = self.analyze_document_structure(file_path)
        if not documents:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {file_path}")
            return []
        
        # –®–∞–≥ 2: –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–ª—è zayavki
        chunks = self.create_zayavki_chunks(documents)
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ zayavki –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks
    
    def process_directory(self, source_dir: str, output_dir: str = None) -> Dict[str, List[DocumentChunk]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö markdown —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
        
        Args:
            source_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–∏–º—è_—Ñ–∞–π–ª–∞: —Å–ø–∏—Å–æ–∫_—á–∞–Ω–∫–æ–≤}
        """
        source_path = Path(source_dir)
        if not source_path.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {source_dir}")
            return {}
        
        markdown_files = list(source_path.glob("*.md"))
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(markdown_files)} markdown —Ñ–∞–π–ª–æ–≤")
        
        all_results = {}
        
        for file_path in markdown_files:
            try:
                chunks = self.process_document(str(file_path))
                if chunks:
                    all_results[file_path.name] = chunks
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ JSON (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –≤—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)
                    if output_dir:
                        self._save_chunks_metadata(chunks, file_path.stem, output_dir)
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        
        logger.info(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_results)} —Ñ–∞–π–ª–æ–≤")
        return all_results
    
    def _save_chunks_metadata(self, chunks: List[DocumentChunk], file_stem: str, output_dir: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        metadata_file = output_path / f"{file_stem}_metadata.json"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º dataclass –≤ dict –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        chunks_data = [asdict(chunk) for chunk in chunks]
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metadata_file}")

def test_document_processing():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –ø—Ä–∏–º–µ—Ä–µ"""
    processor = DocumentProcessor()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 2-3 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    source_dir = "data/markdown_with_headers0"
    
    if not os.path.exists(source_dir):
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {source_dir}")
        return
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_files = []
    for file_name in os.listdir(source_dir):
        if file_name.endswith('.md') and len(test_files) < 2:
            test_files.append(os.path.join(source_dir, file_name))
    
    logger.info(f"üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª–∞—Ö: {[os.path.basename(f) for f in test_files]}")
    
    for test_file in test_files:
        logger.info(f"\n" + "="*60)
        chunks = processor.process_document(test_file)
        
        if chunks:
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {os.path.basename(test_file)}:")
            logger.info(f"   - –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            logger.info(f"   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞: {sum(c.text_length for c in chunks) // len(chunks)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–µ—Ä–∞—Ä—Ö–∏–∏
            unique_hierarchies = set(c.hierarchy_path for c in chunks[:5])
            logger.info(f"   - –ü—Ä–∏–º–µ—Ä—ã –∏–µ—Ä–∞—Ä—Ö–∏–∏:")
            for hierarchy in list(unique_hierarchies)[:3]:
                logger.info(f"     ‚Ä¢ {hierarchy}")

if __name__ == "__main__":
    test_document_processing() 