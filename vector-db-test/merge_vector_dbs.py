#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î –≤ –µ–¥–∏–Ω—É—é –±–∞–∑—É

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç 18 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö FAISS –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –æ–¥–∏–Ω –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞.
"""

import sys
import os
from pathlib import Path
import json
import logging

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from typing import List
from utils.embeddings import SentenceTransformerEmbeddings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def merge_vector_dbs():
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î –≤ –æ–¥–Ω—É"""

    logger.info("=" * 80)
    logger.info("–û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–ï–ö–¢–û–†–ù–´–• –ë–ê–ó –ù–û–†–ú–ê–¢–ò–í–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í")
    logger.info("=" * 80)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    vectordb_dir = project_root / "vector-db-test/vectordb"
    model_name = "intfloat/multilingual-e5-base"
    output_dir = vectordb_dir / "unified_all_docs_e5-base"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if not vectordb_dir.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {vectordb_dir}")
        return False

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ë–î
    db_dirs = sorted([d for d in vectordb_dir.iterdir() if d.is_dir() and not d.name.startswith('.')])

    if not db_dirs:
        logger.error("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        return False

    logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(db_dirs)} –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ë–î")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    model = SentenceTransformer(model_name)
    embeddings = SentenceTransformerEmbeddings(model)
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ë–î
    vector_stores = []
    total_vectors = 0

    logger.info("\n" + "=" * 80)
    logger.info("–ó–ê–ì–†–£–ó–ö–ê –í–ï–ö–¢–û–†–ù–´–• –ë–ê–ó")
    logger.info("=" * 80)

    for i, db_dir in enumerate(db_dirs, 1):
        db_name = db_dir.name
        logger.info(f"\n[{i}/{len(db_dirs)}] –ó–∞–≥—Ä—É–∑–∫–∞: {db_name}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            vector_store = FAISS.load_local(
                str(db_dir),
                embeddings,
                allow_dangerous_deserialization=True
            )

            num_vectors = vector_store.index.ntotal
            total_vectors += num_vectors
            logger.info(f"  ‚îú‚îÄ –í–µ–∫—Ç–æ—Ä–æ–≤: {num_vectors}")
            logger.info(f"  ‚îî‚îÄ ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ")

            vector_stores.append(vector_store)

        except Exception as e:
            logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {db_name}: {e}")
            import traceback
            traceback.print_exc()
            continue

    if not vector_stores:
        logger.error("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î!")
        return False

    logger.info("\n" + "=" * 80)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–™–ï–î–ò–ù–ï–ù–ò–Ø")
    logger.info("=" * 80)
    logger.info(f"–í—Å–µ–≥–æ –ë–î –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(vector_stores)}")
    logger.info(f"–í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {total_vectors}")
    logger.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {vector_stores[0].index.d}")

    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    logger.info("\n" + "=" * 80)
    logger.info("–û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ò–ù–î–ï–ö–°–û–í")
    logger.info("=" * 80)

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ merge_from –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–æ–≤
        unified_store = vector_stores[0]

        logger.info(f"–ë–∞–∑–æ–≤–∞—è –ë–î: {db_dirs[0].name} ({unified_store.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤)")

        for i in range(1, len(vector_stores)):
            db_name = db_dirs[i].name
            vectors_count = vector_stores[i].index.ntotal
            logger.info(f"–î–æ–±–∞–≤–ª—è–µ–º: {db_name} ({vectors_count} –≤–µ–∫—Ç–æ—Ä–æ–≤)...")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
            unified_store.index.merge_from(vector_stores[i].index)

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ docstore –ë–ï–ó –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∏–Ω–¥–µ–∫—Å
            for doc_id in vector_stores[i].index_to_docstore_id.values():
                try:
                    doc = vector_stores[i].docstore.search(doc_id)
                    if doc:
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                        if hasattr(doc, 'metadata'):
                            doc.metadata['source_db'] = db_dirs[i].name
                        # –î–æ–±–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –≤ docstore, –ù–ï –≤ –∏–Ω–¥–µ–∫—Å (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
                        new_doc_id = str(len(unified_store.index_to_docstore_id))
                        unified_store.docstore.add({new_doc_id: doc})
                except Exception as e:
                    logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
                    continue

        logger.info(f"\n‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {unified_store.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –ë–î
        logger.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)

        unified_store.save_local(str(output_dir))
        logger.info("‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report = {
            "unified_db_path": str(output_dir),
            "model": model_name,
            "total_vectors": int(unified_store.index.ntotal),
            "dimension": int(unified_store.index.d),
            "source_dbs": {
                "count": len(db_dirs),
                "names": [d.name for d in db_dirs]
            },
            "created_at": str(Path.cwd())
        }

        report_file = project_root / "vector-db-test/unified_db_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        logger.info("=" * 80)
        logger.info(f"üìÅ –ü—É—Ç—å –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –ë–î: {output_dir}")
        logger.info(f"üìä –í—Å–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {unified_store.index.ntotal}")
        logger.info(f"üì¶ –ú–æ–¥–µ–ª—å: {model_name}")
        logger.info(f"üìÑ –ò—Å—Ö–æ–¥–Ω—ã—Ö –ë–î: {len(db_dirs)}")

        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    success = merge_vector_dbs()
    sys.exit(0 if success else 1)
