#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ Smart Chunker –Ω–∞ –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
"""

import sys
import re
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Any

# –ò–º–ø–æ—Ä—Ç—ã
sys.path.insert(0, str(Path(__file__).parent))
from smart_chunker import SmartDocumentChunker
from universal_chunker import UniversalDocumentChunker

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
FULLDOCS_DIR = Path(__file__).parent / "fulldocx"
REPORTS_DIR = Path(__file__).parent / "reports"
REPORTS_DIR.mkdir(exist_ok=True)


def test_all_documents():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ fulldocx"""

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ DOCX —Ñ–∞–π–ª—ã
    docx_files = sorted(FULLDOCS_DIR.glob("*.docx"))

    print("=" * 100)
    print(f"–ü–†–û–í–ï–†–ö–ê –ß–ê–ù–ö–ï–†–ê –ù–ê –í–°–ï–• –î–û–ö–£–ú–ï–ù–¢–ê–•")
    print("=" * 100)
    print(f"\n–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docx_files)}\n")

    results = []
    errors = []
    successes = []

    for i, docx_file in enumerate(docx_files, 1):
        structure_file = FULLDOCS_DIR / f"{docx_file.stem}_structure.txt"

        if not structure_file.exists():
            errors.append({
                'document': docx_file.name,
                'error': '–§–∞–π–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω',
                'type': 'NO_STRUCTURE'
            })
            print(f"‚ùå [{i}/{len(docx_files)}] {docx_file.name[:60]}")
            print(f"   –ü—Ä–∏—á–∏–Ω–∞: –§–∞–π–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω")
            continue

        try:
            print(f"üîç [{i}/{len(docx_files)}] {docx_file.name[:60]}")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunker = UniversalDocumentChunker(docx_file, structure_file)
            doc_type = chunker.doc_type

            print(f"   –¢–∏–ø: {doc_type}")

            # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ —á–∞–Ω–∫–∏–Ω–≥–∞
            if doc_type == 'CODE':
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Smart Chunker
                smart_chunker = SmartDocumentChunker(docx_file, structure_file)
                chunks = smart_chunker.extract_text_with_structure()
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–µ—Ä
                chunks = chunker.extract_chunks()

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            level_1 = [c for c in chunks if c.get('level') == 1]
            level_2 = [c for c in chunks if c.get('level') == 2]
            other = [c for c in chunks if c.get('level') not in [1, 2]]

            result = {
                'document': docx_file.name,
                'doc_type': doc_type,
                'total_chunks': len(chunks),
                'level_1': len(level_1),
                'level_2': len(level_2),
                'other': len(other),
                'status': 'OK'
            }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
            if chunks:
                sizes = [len(c['text']) for c in chunks]
                result['min_size'] = min(sizes)
                result['max_size'] = max(sizes)
                result['avg_size'] = sum(sizes) // len(sizes)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
                if min(sizes) < 50:
                    result['warning'] = '–û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ (< 50 —Å–∏–º–≤–æ–ª–æ–≤)'
                if max(sizes) > 50000:
                    result['warning'] = '–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏ (> 50000 —Å–∏–º–≤–æ–ª–æ–≤)'

            results.append(result)
            successes.append(result)

            print(f"   –ß–∞–Ω–∫–æ–≤: {len(chunks)} (L1: {len(level_1)}, L2: {len(level_2)})")
            if 'warning' in result:
                print(f"   ‚ö†Ô∏è  {result['warning']}")
            print(f"   ‚úÖ OK")

        except Exception as e:
            error_msg = str(e)
            errors.append({
                'document': docx_file.name,
                'error': error_msg,
                'type': 'EXCEPTION'
            })
            print(f"   ‚ùå –û–®–ò–ë–ö–ê: {error_msg[:100]}")

        print()

    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("=" * 100)
    print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 100)

    print(f"\n–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docx_files)}")
    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(successes)}")
    print(f"–° –æ—à–∏–±–∫–∞–º–∏: {len(errors)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\n" + "=" * 100)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("=" * 100)

    type_stats = defaultdict(list)
    for r in successes:
        type_stats[r['doc_type']].append(r)

    for doc_type, docs in sorted(type_stats.items()):
        total_chunks = sum(d['total_chunks'] for d in docs)
        avg_chunks = total_chunks // len(docs)

        print(f"\n{doc_type}:")
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs)}")
        print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {avg_chunks}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        print(f"   –ü—Ä–∏–º–µ—Ä—ã:")
        for d in docs[:3]:
            print(f"      - {d['document'][:60]}: {d['total_chunks']} —á–∞–Ω–∫–æ–≤")

    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if errors:
        print("\n" + "=" * 100)
        print("–ü–†–û–ë–õ–ï–ú–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´")
        print("=" * 100)

        for err in errors:
            print(f"\n‚ùå {err['document']}")
            print(f"   –¢–∏–ø: {err['type']}")
            print(f"   –û—à–∏–±–∫–∞: {err['error']}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\n" + "=" * 100)
    print("–ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ß–ê–ù–ö–û–í")
    print("=" * 100)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–Ω–æ–º–∞–ª—å–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–µ/–±–æ–ª—å—à–∏–µ —á–∞–Ω–∫–∏
    very_small = [d for d in successes if d.get('min_size', 0) < 50]
    very_large = [d for d in successes if d.get('max_size', 0) > 50000]

    if very_small:
        print(f"\n‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —á–∞–Ω–∫–∞–º–∏ (< 50 —Å–∏–º–≤–æ–ª–æ–≤): {len(very_small)}")
        for d in very_small[:5]:
            print(f"   - {d['document'][:60]}: –º–∏–Ω {d['min_size']} —Å–∏–º–≤–æ–ª–æ–≤")

    if very_large:
        print(f"\n‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏ —á–∞–Ω–∫–∞–º–∏ (> 50000 —Å–∏–º–≤–æ–ª–æ–≤): {len(very_large)}")
        for d in very_large[:5]:
            print(f"   - {d['document'][:60]}: –º–∞–∫—Å {d['max_size']} —Å–∏–º–≤–æ–ª–æ–≤")

    if not very_small and not very_large:
        print("\n‚úÖ –†–∞–∑–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤ –≤ –Ω–æ—Ä–º–µ")

    # –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–µ–∫—Å–æ–≤
    print("\n" + "=" * 100)
    print("–î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–û–î–ï–ö–°–û–í")
    print("=" * 100)

    codes = [d for d in successes if d['doc_type'] == 'CODE']

    for code_result in codes:
        doc_name = code_result['document']
        print(f"\nüìÑ {doc_name}")

        docx_file = FULLDOCS_DIR / doc_name
        structure_file = FULLDOCS_DIR / f"{docx_file.stem}_structure.txt"

        try:
            smart_chunker = SmartDocumentChunker(docx_file, structure_file)
            chunks = smart_chunker.extract_text_with_structure()

            level_1 = [c for c in chunks if c['level'] == 1]
            level_2 = [c for c in chunks if c['level'] == 2]

            print(f"   Level 1 (—Å—Ç–∞—Ç—å–∏): {len(level_1)}")
            print(f"   Level 2 (–ø—É–Ω–∫—Ç—ã): {len(level_2)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º parent-child —Å—Å—ã–ª–∫–∏
            if level_2:
                no_parent = [c for c in level_2 if not c.get('parent_article')]
                if no_parent:
                    print(f"   ‚ö†Ô∏è  {len(no_parent)} level 2 –ë–ï–ó parent_article")
                else:
                    print(f"   ‚úÖ –í—Å–µ level 2 –∏–º–µ—é—Ç parent_article")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º metadata
            missing_metadata = 0
            for c in chunks[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10
                if not c.get('metadata', {}).get('article'):
                    missing_metadata += 1

            if missing_metadata > 0:
                print(f"   ‚ö†Ô∏è  {missing_metadata} —á–∞–Ω–∫–æ–≤ –ë–ï–ó article –≤ metadata")
            else:
                print(f"   ‚úÖ Metadata –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}")

    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\n" + "=" * 100)
    print("–ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê")
    print("=" * 100)

    success_rate = (len(successes) / len(docx_files)) * 100 if docx_files else 0

    print(f"\n–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}% ({len(successes)}/{len(docx_files)})")

    critical_issues = len([e for e in errors if e['type'] in ['EXCEPTION', 'NO_STRUCTURE']])

    if critical_issues == 0 and len(very_large) == 0:
        print("\n‚úÖ‚úÖ‚úÖ –í–°–ï –î–û–ö–£–ú–ï–ù–¢–´ –û–ë–†–ê–ë–û–¢–ê–ù–´ –£–°–ü–ï–®–ù–û!")
        print("‚úÖ –ß–∞–Ω–∫–µ—Ä –†–ê–ë–û–¢–ê–ï–¢ –ò–î–ï–ê–õ–¨–ù–û –Ω–∞ –≤—Å–µ—Ö —Ç–∏–ø–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
        print("‚úÖ –ú–û–ñ–ù–û –°–û–ó–î–ê–í–ê–¢–¨ –í–ï–ö–¢–û–†–ù–£–Æ –ë–ê–ó–£!")
        return True
    else:
        print(f"\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {critical_issues} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö, {len(very_large)} –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤")
        return False


if __name__ == '__main__':
    success = test_all_documents()
    sys.exit(0 if success else 1)
